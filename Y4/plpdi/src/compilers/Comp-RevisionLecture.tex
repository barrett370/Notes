\documentclass{article}

\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{forest}

\usetikzlibrary{arrows,shapes,automata,petri,positioning,calc}
\tikzset{
    fsa-point/.style={
        circle,
        draw=black,
        minimum size=3pt
    },
    every edge/.style={
        draw,
        ->,>=stealth',
        auto,
        semithick
    },
    initial text = {},
    double distance=2pt,
}
\renewcommand{\i}[1]{\textit{#1}}
\renewcommand{\u}[1]{\underline{#1}}
\newcommand{\rarr}{\rightarrow}

\title{PLPDI Compilers: Revision Lecture}
\author{Sam Barrett}

\begin{document}
\maketitle

Compiler books often focus on the task of compiling the C programming language. This was intentionally not the focus of this submodule.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.25\linewidth]{CompilerFlow.png}
    \caption{Compiler Flow}%
    \label{fig:compFlow}
\end{figure}
\section{Lexical Analysis}

Lexical analysis, the job performed by the \textit{Lexer} is the first stage of compilation. It converts a source file written in a specific programming language into a string of \i{tokens}. Or alternatively, string $\implies$ list of "lexemes"

For example, the code:


\begin{align*}
    \texttt{fold (+) [1;2] 0} &\implies \\ 
                              &\texttt{identifier open-round-bracket operator } \\ 
                              &\texttt{close-round-bracket open-square-bracket constant } \\ &\texttt{semicolon constant close-square-bracket constant}
\end{align*}


Lexemes are specified using regular expression which, in turn, are implemented using finite state automata.

For example, a simple Regular Expression capturing the \i{lexeme} of numbers:

$$
num = \texttt{[+-]?[1-9][0-9]}^\ast
$$

Backtracking is the \i{dumb} approach to parsing regular expressions. It is also known as the brute force method as in the worst case scenario, all possibilities could be tried before the approach \i{gives up}.

Such an expression which is tricky to parse using backtracking is:

$$
tricky = \texttt{a?a?aa}
$$

Every occurrence of the \i{optional}, ?, doubles the number of possible routes through a parse tree of this expression. Meaning an input such as $ \texttt{aaaaa}$ would cause an evaluator to explore every possible branch of this tree before determining that it is not captured.

\subsection{Finite State Automata}

A better approach is to use Finite state automata. This approach was originally proposed by Ken Thompson. 

Using FSA our $tricky$ regex can be converted to the following Deterministic Finite State Automaton or DFA. 
\begin{center}
    \begin{tikzpicture}
        \node [fsa-point,initial] (P1) {0};
        \node [fsa-point,right of=P1] (P2) {3};
        \node [fsa-point,accepting,right of=P2] (P3) {4};
        \node [fsa-point,accepting,right of=P3] (P4) {2};
        \node [fsa-point,accepting,right of=P4] (P5) {1};
        \node [fsa-point,right of=P5] (P6) {5};
        \draw (P1) edge node {a} (P2);
        \draw (P2) edge node {a} (P3);
        \draw (P3) edge node {a} (P4);
        \draw (P4) edge node {a} (P5);
        \draw (P5) edge node {a} (P6);
        \draw (P6) edge[loop above] node {a} (P6);
    \end{tikzpicture}
\end{center}

However, the process to create a DFA is computationally expensive. Therefore, Thompson's algorithm instead operates over Non-deterministic finite state automata. It works by keeping track of all the positions in the DFA where we could be given the string we have seen so far.

\section{Parsing}

In this stage of the process where the sequence of tokens generated by the lexer is transformed into the Abstract Syntax Tree (ASG) of the program. An ASG is a record of the grammatical structure of the program and is essential to the correct interpretation of a program. 

\subsection{Grammars}

A grammar is a set of rules. They are defined using Terminals and Non-Terminals. They are differentiated by the fact that during every application of a rule, the non-terminal occurring on the left-hand side (see below) is replaced by a sequence of terminals and non-terminals appearing on the right-hand side.

An example grammar could be:
$$
E \rightarrow N | E + E | E * E
$$

When used as a \i{production system} our rule(s) is(are) repeatedly applied to a designated starting symbol until we produce an expression comprised exclusively of terminal symbols.

For example:

\begin{align*}
    E &\rarr \u{E} + E &&\texttt{apply } E \rarr E + E \\
      &\rarr \u{E} * E + E &&\texttt{apply } E \rarr E * E \\
      &\rarr 42 * \u{E} + E &&\texttt{apply } E \rarr N \\
      &\rarr 42 * 35 + \u{E} && \texttt{apply } E \rarr N \\
      &\rarr 42 * 35 + 17 && \texttt{apply } E \rarr N
\end{align*}

We can also run this process in reverse to convert a string of terminal symbols back to the starting non-terminal $E$. 

We can however, recognise an issue with this process whereby, the set of possible operations at any given point is greater than 1. i.e. the process is \u{not} deterministic.

For example:

\begin{center}
   \begin{forest}
       [E [E [E [ 42] ]
             [*]
             [E [ 35] ]
          ]
          [+]
          [E [ 17] ] 
        ]
   \end{forest} 
\end{center}

\begin{center}
   \begin{forest}
       [E [ E [ 42] ]
          [ *]
          [ E [
                [E [ 35]]
                [+]
                [E [17]]
                ]
            ]
            ]
   \end{forest} 
\end{center}

These trees are both valid under this grammar but they are not both valid mathematically. This is because the grammar does not embed the \i{strength} of the operators.

$$
(42 * 35) + 17 \neq 42 * (35 + 17)
$$

Ambiguity and non-determinism are some of the biggest issues faced when constructing a grammar indented for use as a production system or parsing. It has been shown that the ambiguity of context-free grammars is an un-decidable problem, i.e. there is no way to construct a program that can \i{decide} whether a given grammar is ambiguous. This leads to the designing of grammars more of an \i{art} relying on the experience of the creator and the use of heuristics.

We can re-write our original grammar in a non-ambiguous way to prevent the formulation of strings that can have conflicting parse trees.


\begin{align*}
    E &\rarr S + S | S \\
    S &\rarr M * M | M \\
    M &\rarr ( E ) | N 
\end{align*}

\begin{center}
\begin{forest}
    [E [S [M [42]]
                  [*]
                  [M [35]]
                ]
              [+]
              [S [M [17]]]
              ]
\end{forest}
\end{center}

Above you can see that our valid example can be constructed using this new grammar but below you can see that the semantically invalid tree is now also structurally invalid: 

\begin{center}
   \begin{forest}
        [ ?? [ S [ M [ 42 ] ] ]
             [ *]
             [ E [ S [ M [ 35] ] ]
                 [ + ]
                 [ S [ M [ 17 ] ] ]
                ]
            ]
   \end{forest} 
\end{center}

\subsection{Top-down parsing}
\subsubsection{Recursive Descent}
Recursive descent means we start at the \u{top} with our starting non-terminal, in this case $E$. We then apply rules until we reach a string comprised entirely of terminals, if the terminals match the string we are trying to generate then the string is valid otherwise we backtrack and apply different rules. We repeat this process until we either exhaust all possible valid combinations of rules or we match the terminals.

From the section on Lexical Analysis we know that backtracking is rarely a good solution as it has very poor worst-case complexity (exponential).

\subsubsection{LL($k$) Parsing}
There is an improved version of top-down parsing called LL(k) or left-to-right leftmost derivation with k tokens lookahead. The benefit of a \i{lookahead} is that it is able to disambiguate the application of the rule. The caveat is that the grammar has to be written in a compatible way for this system to work.

The LL(1) form of our grammar is formulated as such:

\begin{align*}
    E &\rarr T E' \\
    E' &\rarr + E | \varepsilon \\
    T &\rarr F T' \\
    T' &\rarr * T| \varepsilon \\
    F &\rarr N | ( E )
\end{align*}

Grammars of this type can be constructed algorithmically from an already deterministic grammar. \textbf{However,} not all grammars can be put into LL(1) form. It has been shown that it is undecidable whether given a grammar there is a fixed value of $k$ s.t. the grammar can be put into LL($k$) form.

\subsubsection{Monadic Parsing}

In order to avoid backtracking we build on the fact that we have seen that we can often replace time complexity with space complexity.

Simply speaking, in this approach we remember, inside of a data structure, the choices we have not made. i.e. in a list we store all the possible rules and evaluate all of them at the same time, discarding invalid trees until we either run out of trees or find one that is valid.
\subsection{Bottom-up parsing:}
\subsubsection{LR($k$): left-to-right rightmost derivation with $k$ tokens lookahead (LALR)}

Here we start at the \i{bottom} with our terminal string and apply rules until we reach our starting non-terminal. We again utilise $k$ lookahead to disambiguate rules as we apply them, this allows us to discount a rule earlier if we see that it doesn't reduce to a required intermediary form.

There are a few problems which can arise when using LALR parsers. 

One of these issues is known as a shift-reduce conflict. A typical example of a shift-reduce conflict is if-then-else in languages that permit both if-then and if-then-else syntactic structures. For example:

$$
\texttt{if x then if y then a else b}
$$
Is this mean to be processed as:

$$
\texttt{if x then \{if y then a\} else b}
$$

or 

$$
\texttt{if x then \{if y then a else b\}}
$$

These are usually disambiguated by the parser mechanism preferring the \i{shift} to the \i{reduce}. Essentially, always trying to construct the longest possible parses.

A more problematic type of conflict is the reduce-reduce conflict.

\begin{align*}
    \texttt{Seq} &\rarr \varepsilon \\
        &| \texttt{Maybe} \\
        &| \texttt{Seq }a \\
        \\
    \texttt{Maybe} &\rarr \varepsilon \\
          &| a
\end{align*}

Here we can derive $a$ with 2 different parse trees, either via the Seq structure or Maybe structure.

When you have one of these conflicts it is unclear which operation is going to happen and generally the execution order boils down to the order of the rules in the grammar definition.

\textbf{Note: both of these conflicts are typically unavoidable in a large grammar, mitigating them is an art rather than a science.} 

Larger parsers are generally not written by hand and are instead generated using a \textbf{Parser generator} such as \i{YACC, Bison, Parsec, etc.}

\section{Intermediate Representation}

\subsection{Abstract Syntax Trees (ASTs)}

After the parse tree is created, it needs to be processed further so that it is easier to execute/compile. 
It is easier to initially consider interpretation. We want to, given an abstract syntax tree, interpret the expression that the Abstract syntax graph represents.

For example given the input string:

$$
(1 + 2) + (3 + 4)
$$ 

We token-ise it to get:

$$
( \;\; \u{1}\;\;  \u{+}\;\;  \u{2}\;\;  )\;\;  \u{+}\;\;  (\;\;  \u{3}\;\;  \u{+}\;\;  \u{4}\;\;  )
$$

where underlined values are individual tokens.

We then construct a parse tree of this tokenised string:

\textbf{Note: the names of the terminals/non-terminals is unimportant} 

\begin{center}
    \begin{forest} for tree={grow=north, circle, draw, parent anchor=north, child anchor=south,}
    [ 
            [
                [ \u{4}]
                [ \u{+}]
                [ \u{3}]
            ]
           [[\u{+}]]
            [
                [\u{2}]
                [\u{+}]
                [\u{1}]
           ]
    ]
    \end{forest} 
\end{center}

Notice how we have removed the brackets; once the tree has been created, the brackets become implicit.
We can also push the operators onto the parent nodes to produce an equivalent but prettier Abstract syntax tree: 

\begin{center}
    \begin{forest} for tree={grow=north, circle, draw, parent anchor=north, child anchor=south}
     [+ 
        [+ 
            [\u{4}]
            [\u{3}]
        ]
        [+ 
            [\u{2}]
            [\u{1}]
        ]
     ]
    \end{forest} 
\end{center}

We can show program execution as transformations on these Abstract Syntax Trees. 
How do we know what order to apply these transformations? We use a fixed traversal method such as Depth first left-right traversal.

The application of this form of traversal on our exemplar can be seen in Figure~\ref{fig:AST1}

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.6\linewidth]{c4-2.jpg}
    \caption{Depth-first L-R traversal}%
    \label{fig:AST1}
\end{figure}

\subsection{Note from Revision Lecture}
We must be aware of the distinction between a compiler evaluation and optimisation.

Abstractly, a evaluation is a set of abstract syntax graph transformations applied according to a schedule. For a call-by-value programming language this takes the form of a depth first search traversal of the ASG with reductions applied on the path \textit{back} up the tree towards the root.

A compiler optimisation is applying some transformations that make the code simpler and faster in an arbitrary order

A compiler optimisation is applying some transformations that make the code simpler and faster in an arbitrary order. Optimisations often have conditions that have to be met for them to be applied.
An example of a compiler optimisation is closure conversion. Closures being anonymous functions.

In a closure conversion we want to \textit{pull} inner (anonymous) functions into global scope. However, this raises an issue: what do we do with the variables of the closure which are bound in the enclosing function? We solve this using a notion of environment and transforming all functions in a uniform way.


\section{Types}



\section{Assignment}

\section{Code Generation}

\begin{displayquote}
   'Abstract machines give you the cake.'\\
   'Compilers give you the recipe.'\\
   ---Dan Ghica
\end{displayquote}

Only touched on briefly due to complexity. Essentially, code generation is the final process of compilation. If Abstract machines execute the program, Compilers (via code generation) say what to do. 

\section{Tips for the Exam}
\begin{enumerate}
    \item Very similar to the summative assignment.
    \item Use \textit{SPARTAN}
\end{enumerate}


\end{document}
