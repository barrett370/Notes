<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="My University Notes Site.">
<title>
 - Welcome to my Notes Repo!
</title>




<link rel="shortcut icon" href="/Notes/sam.ico">








<link rel="stylesheet" href="/Notes/css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sam-barrett.codes/Notes/tn.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Lecture 10 Model Capacity Informally, the Model Capacity is a model&rsquo;s capacity to fit a wide range of functions. In statistical learning theory, model capacity is quantified by *VC-Dimension: Largest training set for which the model cna classify the labels arbitrarily into two classes By the universal approximation theorem, neural networks can have a very high capacity see previous lecture
Underfitting &amp; Overfitting Underfitting: Too high a training error Overfitting: Too large a gap between training error and test error"/>

<meta property="og:title" content="" />
<meta property="og:description" content="Lecture 10 Model Capacity Informally, the Model Capacity is a model&rsquo;s capacity to fit a wide range of functions. In statistical learning theory, model capacity is quantified by *VC-Dimension: Largest training set for which the model cna classify the labels arbitrarily into two classes By the universal approximation theorem, neural networks can have a very high capacity see previous lecture
Underfitting &amp; Overfitting Underfitting: Too high a training error Overfitting: Too large a gap between training error and test error" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sam-barrett.codes/Notes/modules/neural-computation/source/neural-comp-lecture10/" />

<meta property="og:image" content="https://sam-barrett.codes/Notes/tn.png" />
<meta property="og:site_name" content="Notes" />


    

    
    
    
    <title>
        
        
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title"></div>

        <div class="section" id="content">

<h1 id="lecture-10">Lecture 10</h1>

<h2 id="model-capacity">Model Capacity</h2>

<p>Informally, the <em>Model Capacity</em> is a model&rsquo;s capacity to fit a wide range of functions.
In statistical learning theory, <em>model capacity</em> is quantified by *<strong>VC-Dimension</strong>: Largest training set for which the model cna classify the labels arbitrarily into two classes
By the universal approximation theorem, neural networks can have a very high capacity <a href="../out/Neural-Comp-Lecture9.html">see previous lecture</a></p>

<h2 id="underfitting-overfitting">Underfitting &amp; Overfitting</h2>

<p>Underfitting: Too high a training error
Overfitting: Too large a gap between training error and test error</p>

<h2 id="model-capacity-vs-error">Model Capacity vs Error</h2>

<p>Training and test error behave differently</p>

<ul>
<li>Training error often decreases with capacity</li>
<li>Test error can increase beyond a certain capacity</li>
</ul>

<p>The capacity of a model is optimum when the model matches data generation process</p>

<h2 id="regularisation">Regularisation</h2>

<p>There are 3 model <em>regimes</em></p>

<ol>
<li>Model family excludes data-generating process (<em>underfitting</em>)</li>
<li>Model family matches data-generating process</li>
<li>Model family matches data-generating process, and possibly many other models (<em>possible overfitting</em>)</li>
</ol>

<p>Regularisation attempts to move a model from 2 $\rightarrow$ 3</p>

<h3 id="data-augmentation">Data augmentation</h3>

<p>Many datasets can be augmented via transformations
Examples include:</p>

<ul>
<li>Mirroring</li>
<li>Translation</li>
<li>Scaling</li>
<li>Rotation</li>
<li>Noise</li>
</ul>

<h3 id="regularisation-methods">Regularisation Methods</h3>

<h4 id="early-stopping">Early Stopping</h4>

<p>Here we split the data into training, validation and test data</p>

<ul>
<li>Training model on the training set, evaluate with fixed intervals on the validation set</li>
<li>Stop training when validation error has increased</li>
<li>Return model parameters when validation loss was at the lowest, rather than the last parameters</li>
</ul>

<h4 id="parameter-norm-penalties">Parameter Norm Penalties</h4>

<p>Replace cost function with:</p>

<p>$$
\widetilde{C}(\Theta; X,y) = C(\Theta;X,y) + \alpha\Omega (\Theta)
$$</p>

<p>Where:</p>

<ul>
<li>$C$ is the original cost function</li>
<li>$\Theta$ is the model&rsquo;s parameters</li>
<li>$X,y$ is the training data</li>
<li>$\Omega$ is a regular-iser i.e. a function which penalises complex models</li>
<li>$\alpha$ is a hyperparameter controlling the degree of regularisation</li>
</ul>

<h4 id="l-2-parameter-regularisation">$L^2$ Parameter Regularisation</h4>

<p>Assuming parameters are weights and biases, i.e. $\Theta= (w,b)$
Then we can define:</p>

<p>$$
\Omega(\Theta) := \frac{1}{2}|w|_2^2
$$</p>

<p>This allows us to penalise large weights</p>

<h4 id="ensemble-methods">Ensemble Methods</h4>

<p>Combining different models often reduces generalisation error.</p>

<h5 id="idea">Idea</h5>

<p>Train $k$ Neural Networks on $k$ subsets of the training data. Output the average (or modal) value of the networks</p>

<h5 id="disadvantages">Disadvantages</h5>

<ul>
<li>Usually requires more training data</li>
<li>$k$ times increase in training time (if sequentially training)</li>
<li>Only feasible for small values of $k$</li>
</ul>

<h5 id="idea-2-dropout">Idea 2: Dropout</h5>

<p>In each <em>mini-batch</em>, <em>deactivate</em> some randomly selected activation units (not in the output layer)</p>

<p>Each selection of units corresponds to a sub-network.
With $n$ inputs and hidden layer activation units, there are $2^n$ sub-networks.</p>

<p>The sub-networks share the weights.</p>

<p>No dropout during testing, i.e. implicit average output from all sub-networks</p>

<p>@import &ldquo;../resources/l10-dropout.png&rdquo;</p>

<h6 id="implementing-dropout">Implementing Dropout</h6>

<p>Replace each activation unit $a_j^l = \phi(z_j^l)$ in a <em>hidden layer</em> with a dropout activation unit.</p>

<p>$$
\widetilde{a}_j^l = \frac{1}{1-p}\cdot d_j^l \cdot \phi(z_j^l)
$$</p>

<p>Where:</p>

<ul>
<li>$d_j^l \sim \text{Bernoulli}(1-p)$

<ul>
<li>Is 0 with Probability $p$ and 1 otherwise.</li>
</ul></li>
</ul>

<p>@import &ldquo;../resources/l10-dropoutimp.png&rdquo;</p>

<p>The expected value of the random variable $\widetilde{a}_j^l$ is:</p>

<p>$$
\mathcal{\mathbb{E}}[\widetilde{a}_j^l] = p\cdot \frac{1}{1-p}\cdot 0 \cdot \phi(z_j^l) + <br />
(1-p) \cdot \frac{1}{1-p} \cdot 1 \cdot \phi(z_j^l) <br />
= \phi(z_j^l) = a_j^l
$$</p>

<p>Therefore, choosing the factor $\frac{1}{1-p}$ makes the expected activation identical to the activation without dropout</p>

<h6 id="backpropogation-with-dropout">Backpropogation with Dropout</h6>

<p>@import &ldquo;../resources/l10-backpropdropout.png&rdquo;</p>

<p>Given the local gradient $\delta_j^l$, partial derivatives are:</p>

<p>$$
\frac{\delta C}{\delta w_{jk}^k} = \frac{\delta C}{\delta z_j^l} \cdot \frac{\delta z<em>j^l}{\delta w</em>{jk}^l } \ = \delta_j^l \cdot \widetilde{a}_k^{l-1}
$$</p>

<p>$$
\frac{\delta C}{\delta b_j^l} = \frac{\delta C}{\delta z_j^l} \cdot \frac{\delta z_j^l}{\delta b_j^l} \ = \delta_j^l
$$</p>

<h6 id="local-gradient-for-hidden-layers">Local Gradient for Hidden Layers</h6>

<p>@import &ldquo;../resources/l10-lghl.png&rdquo;</p>

<p>$$
\widetilde{a}_j^l = \frac{1}{1-p}\cdot d_j^l \cdot \phi(z_j^l)
$$</p>

<p>Derivation:</p>

<p>$$
\delta_j^l = \frac{\delta C}{\delta z_j^l}   \;\;\;\;\;\;\; \texttt{By definition}\
= \frac{\delta C}{\delta \widetilde{a}_j^l} \cdot \frac{\delta \widetilde{a}_j^l}{\delta z<em>j^l} \;\;\;\;\;\;\; \texttt{Chain Rule}<br />
= \left( \sum</em>{k=1}^{m}\frac{\delta C}{\delta z_k^{l+1}\cdot\frac{\delta z_j^{l+1}}{\delta \widetilde{a}_j^l }} \right) \cdot \left(  \frac{1}{1-p}\right) \cdot d_j^l \cdot \phi &lsquo;(z<em>j^l) \;\;\;\;\;\;\; \texttt{Chain Rule} <br />
= \left( \sum</em>{k=1}^{m}\delta<em>k^{l+1}\cdot w</em>{kj}^{l+1} \right) \cdot \left( \frac{1}{1-p} \right) \cdot d_j^l \cdot \phi &lsquo;(z_j^l) \;\;\;\;\;\;\; \texttt{By definition of $\delta_j^{l+1}$}
$$</p>

<hr />

<p>Or in Matrix Form:</p>

<p>$$
\delta^l = \left( \frac{1}{1-p} \right)\left( (w^{l+1})^T \delta^{l+1} \right) \odot d^l \odot \phi &lsquo; (z^l)
$$</p>

<h6 id="backpropogation-algorithm-with-dropouts">Backpropogation Algorithm with Dropouts</h6>

<p>$$
\texttt{
    \underline{input}: A training example $(x,y) \in \R^m \times \R^{m&rsquo;}$
}
$$</p>

<ol>
<li>Set the activation in the input layer:</li>
</ol>

<p>$$
d_j^1 \sim \text{Bernoulli}(p), \text{for } j=1,\ldots,m \
\widetilde{a}^1 = \frac{1}{1-p}\cdot d^1\odot x
$$</p>

<ol>
<li>For each $l=2,\ldots (L-1)$  feed forward</li>
</ol>

<p>$$
d_k^l \sim \text{Bernoulli}(p), \text{for } j=1,\ldots, m \
z^l = w^l\widetilde{a}^{l-1}+b^l <br />
\widetilde{a}^l = \frac{1}{1-p}d^l\odot \phi(z^l)
$$</p>

<ol>
<li>Set activations in output layer, <strong>no dropout</strong></li>
</ol>

<p>$$
z^L = w^L\widetilde{a}^{l-1} + b^l <br />
a^L = \phi(z^L)
$$</p>

<ol>
<li>Compute local gradient for output layer:</li>
</ol>

<p>$$
\delta^L := \nabla_a C\odot \phi &lsquo;(z^L)
$$</p>

<ol>
<li>Backpropogate the local gradients for hidden layers, i.e. for e ach $l=L-1 \ldots 2$ :</li>
</ol>

<p>$$
\delta^l := \frac{1}{1-p}\left( (w^(l+1)^T\delta^{l+1}) \right)\odot d^l \odot \phi &lsquo; (z^l)
$$</p>

<ol>
<li>return the partial derivatives</li>
</ol>

<p>$$
\frac{\delta C}{\delta w_{jk}^l} = \delta_j^l\widetilde{a}_k^{l-1} \
\frac{\delta C}{\delta b_j^l} = \delta_j^l
$$</p>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="/Notes/modules/neural-computation/">Neural Computation</a>
        
    
    
        
            &#183; 
            <a href="/Notes/modules/machine-learning/">Machine Learning</a>
        
            &#183; 
            <a href="/Notes/modules/parallel-distributed/">Distributed and Parallel Computing</a>
        
    
    &#183; 
    <a href="https://sam-barrett.codes/Notes/">
        main
    </a>

</p></div>
        

        <div class="section footer"></div>
    </div>
</body>

</html>