<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="My University Notes Site.">
<title>
 - Welcome to my Notes Repo!
</title>




<link rel="shortcut icon" href="/Notes/sam.ico">








<link rel="stylesheet" href="/Notes/css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sam-barrett.codes/Notes/tn.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Lecture 7 2 Types of Parallelism  Task based  e.g. multiply or add suitable for standard multi-core CPUs or networks of computers i.e. have the same code running on multiple cores or CPUs  Data based  e.g. alter an image by performing hte same operation on each pixel in parallel Suitable for GPUs   Latency vs. Throughput  Latency oriented processors
 Minimises delay on first result being returned e."/>

<meta property="og:title" content="" />
<meta property="og:description" content="Lecture 7 2 Types of Parallelism  Task based  e.g. multiply or add suitable for standard multi-core CPUs or networks of computers i.e. have the same code running on multiple cores or CPUs  Data based  e.g. alter an image by performing hte same operation on each pixel in parallel Suitable for GPUs   Latency vs. Throughput  Latency oriented processors
 Minimises delay on first result being returned e." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sam-barrett.codes/Notes/modules/parallel-distributed/source/parallel-dist-lecture7/" />

<meta property="og:image" content="https://sam-barrett.codes/Notes/tn.png" />
<meta property="og:site_name" content="Notes" />


    

    
    
    
    <title>
        
        
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title"></div>

        <div class="section" id="content">

<h1 id="lecture-7">Lecture 7</h1>

<h2 id="2-types-of-parallelism">2 Types of Parallelism</h2>

<ul>
<li>Task based

<ul>
<li>e.g. multiply or add</li>
<li>suitable for standard multi-core <strong>CPUs</strong> or networks of computers</li>
<li>i.e. have the same code running on multiple cores or <strong>CPUs</strong></li>
</ul></li>
<li>Data based

<ul>
<li>e.g. alter an image by performing hte same operation on each pixel in parallel</li>
<li>Suitable for <strong>GPUs</strong></li>
</ul></li>
</ul>

<h2 id="latency-vs-throughput">Latency vs. Throughput</h2>

<ul>
<li><p>Latency oriented processors</p>

<ul>
<li>Minimises delay on <strong>first</strong> result being returned</li>
<li>e.g. if an operation takes 4 cycles, 10 operations will take 40 cycles</li>
<li>Large Caches to speed up memory access</li>
<li>Temporal/ Spacial locality, working sets</li>
<li>Complex control units</li>
<li>Short pipelines, branch prediction, data forwarding

<ul>
<li>to prevent pipeline stalls</li>
</ul></li>
<li>Complex, energy expensive ALUs</li>
<li>To minimise cycles to return a result.
<br /></li>
</ul></li>

<li><p>Throughput oriented processors</p>

<ul>
<li>Minimises delay on <strong>all</strong> results being returned</li>
<li>e.g. implement pipelined operation on ALUs, 10 cycles for first operation and to fill the pipeline but 1 cycles for all subsqeuent operations, total 19 cycles</li>
<li>Small caches</li>
<li>For <em>staging</em> data

<ul>
<li>get blocks of data at one time for a group of threads to work on</li>
<li>avoids each thread making separate fetches</li>
</ul></li>
<li>Simple control units</li>
<li>No branch predction or data forwarding</li>
<li>The control is shared between threads working on different data</li>
<li>Simple, energy efficient ALU</li>
<li>Long pipelines</li>
<li>Large number of cycles per operation but heavily pipelined

<ul>
<li>$\rightarrow$ long wait for first result as pipeline needs to be filled</li>
<li>but following results come quickly</li>
<li>Required large amount of threads to keep processor occupied</li>
</ul></li>
</ul></li>
</ul>

<h6 id="fig-1-von-neumann-architecture">Fig. 1. Von Neumann Architecture</h6>

<div style="text-align:center"><img src="../resources/von-neumann.jpg" /></div>

<h6 id="fig-2-von-neumann-architecture-for-gpu">Fig. 2. Von Neumann Architecture for GPU</h6>

<div style="text-align:center"><img src="../resources/vn-gpu.png" /></div>

<h2 id="compiling-for-cuda">Compiling for CUDA</h2>

<ul>
<li>The host CPU and GPU are separate devices connected by a bus</li>
<li>Each have separate memory</li>
<li>Therefore, we need to generate separate code for each device

<ul>
<li>The nvidia compiler for CUDA programs is <code>nvcc</code></li>
</ul></li>
<li><code>nvcc</code> takes <em>C/C++</em> code for with Nvidia extensions, separates and compiles the GPU code and passes the CPU code to the host to be compiled</li>
<li>This results in a single binary with both CPU and GPU code, the GPU code is loaded onto it when the host runs this.</li>
</ul>

<h2 id="vectoradd-trivial-example">VectorAdd - trivial example</h2>

<p>$$\vec C = \vec A + \vec B$$</p>

<p>The sequential code for this may look like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#66d9ef">void</span> <span style="color:#a6e22e">vecAdd</span>(<span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> h_A, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> h_B, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> h_C, <span style="color:#66d9ef">int</span> n){
    <span style="color:#66d9ef">for</span> (i<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>; i<span style="color:#f92672">&lt;</span>n ; i<span style="color:#f92672">++</span>){
        h_C[i] <span style="color:#f92672">=</span> h_A[i] <span style="color:#f92672">+</span> h_B[i];
    }
}
<span style="color:#66d9ef">int</span> <span style="color:#a6e22e">main</span>(){
    <span style="color:#75715e">// declarations
</span><span style="color:#75715e"></span>    vecAdd(h_A,h_B, h_C, N);
    ...   
}</code></pre></div>
<p>To write this for CUDA we must identify whether a function runs on the host, the device or both.
We also have to work out where it should be callable from:</p>

<ul>
<li>the host: <code>__host__ void f(...)</code>

<ul>
<li>This is default so can be omitted</li>
<li>Callable from the <strong>host only</strong></li>
</ul></li>
<li>The device: <code>__global__ void f(...)</code>

<ul>
<li>special functions called <strong>kernel functions</strong></li>
<li>callable from <strong>host only</strong>. This is how we run code on the GPU</li>
</ul></li>
<li>The device: <code>__device__ void f(...)</code>

<ul>
<li>callable from the <strong>device only</strong>. they are helper functions</li>
</ul></li>
<li>both: <code>__host__ __device__ void f(...)</code></li>
</ul>

<h2 id="cpu-computational-unit-structure">CPU computational unit structure</h2>

<ul>
<li>When we call a kernel function we need to specify how the threads should be organised to execute it

<ul>
<li>Every thread is going to exectue the same kernel.</li>
<li>Different CPU devices can support different numbers of simultaneously executable threads</li>
<li>Within a GPU thread structure is <strong>not</strong> uniform, i.e. do not have equal access to all the GPU memory, syncronise with other processing units or share cache memory</li>
</ul></li>
</ul>

<h2 id="cuda-thread-issues">CUDA thread issues</h2>

<ul>
<li>We don&rsquo;t want the fixed number of GPU threads to dictate the size of the largest vectors we can add</li>
<li>We don&rsquo;t want to have to change our code to run on different GPUs</li>
<li>We need to organise our threads to co-locate groups of threads on sets of processing units to take advantage of shared caches and syncronisation facilities</li>
<li>NVidia GPUs accomplish this by organising threads into a hierarchical structure

<ul>
<li>A <em>Grid</em> is a collection of <em>Blocks</em></li>
<li>A <em>Block</em>  is a collection of <em>Threads</em></li>
<li>A <em>Thread</em> is the execution of a <em>kernel</em> on a single processing unit</li>
</ul></li>
</ul>

<h2 id="cuda-thread-organisation">CUDA thread organisation</h2>

<p>Outside the <em>Grid/Block/THread</em> hierarchy, there is the concept of a <em>Warp</em></p>

<ul>
<li>A <em>Warp</em> is a set of a number of tightly related threads that must execute fully in lock step with each other.</li>
<li>Warps are not part of CUDA but are on all modern Nvidia GPUs, dictated by low level hardware design</li>
<li>The number of threads in a <em>warp</em> is a feature of a particular GPU, but is most commonly 32</li>
<li>Warps are the low-level basis of thread scheduling on a GPU, if a thread is scheduled to execute, so are all other threads in the <em>warp</em></li>

<li><p>As they execute the same instructions in lock step, all threads in a <em>warp</em> will have the same instruction timing</p></li>

<li><p>A block can have a size between 1 and the number of threads on the GPU. (typically 2014) and is the high-level basis of thread scheduling</p></li>

<li><p>Because of the nature of <em>warps</em>, the block size should be a multiple of the <em>warp</em> size</p></li>

<li><p>Grids can have large numbers of blocks, many more than can be concurrently executed.</p></li>
</ul>

<h2 id="invoking-kernel-functions">Invoking Kernel Functions</h2>

<ul>
<li><p>We need to specify the grid/block structure hwen invoking a kernel function</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c">...
<span style="color:#66d9ef">int</span> threadsPerBlock <span style="color:#f92672">=</span> <span style="color:#ae81ff">256</span>;
<span style="color:#66d9ef">int</span> blocksPerGrid <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> (numElements <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> threadsPerBlock;
vectorAdd<span style="color:#f92672">&lt;&lt;&lt;</span>blocksPerGrid, threadsPerBlock <span style="color:#f92672">&gt;&gt;&gt;</span>(d_A, d_B, d_C, numElements);
...</code></pre></div></li>
</ul>

<p><strong>Note <code>&lt;&lt;&lt;blocksPerGrid, threadsPerBlock &gt;&gt;&gt;</code> is not standard C/C++ and is handled by <code>nvcc</code></strong></p>

<h2 id="inside-kernel-functions">Inside Kernel Functions</h2>

<p>Each thread needs to know which part of the data to work on.
CUDA provides predefined variables for this purpose:</p>

<ul>
<li><code>blockIdx.x</code> the unique identifier for this block in this grid</li>
<li><code>blockDim.x</code> the number of threads in a block for this grid</li>

<li><p><code>threadIdx.x</code> the unique identifier of this thread in this block</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c">__global__ <span style="color:#66d9ef">void</span> 
<span style="color:#a6e22e">vectorAdd</span>(<span style="color:#66d9ef">const</span> <span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>A, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>B, <span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>C, <span style="color:#66d9ef">int</span> n){
<span style="color:#66d9ef">int</span> i <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> blockIdx.x <span style="color:#f92672">+</span> threadIdx.x;

<span style="color:#66d9ef">if</span> (i <span style="color:#f92672">&lt;</span> n){
    C[i] <span style="color:#f92672">=</span> A[i] <span style="color:#f92672">+</span> B[i];
}
}</code></pre></div></li>
</ul>

<h2 id="grid-and-block-dimensionalities">Grid and Block Dimensionalities</h2>

<ul>
<li>Grids and Blocks can be organised as 1D, 2D or 3D spaces</li>
<li>Hence the predefined variables <code>blockIdx.x, blockDim.x, threadIdx.x</code> have <code>.y</code> and <code>.z</code> variants

<ul>
<li>If you are using 1D grids and blocks then you can ignore these.</li>
</ul></li>
</ul>

<h2 id="device-global-memory">Device Global Memory</h2>

<ul>
<li>GPU memory is <strong>not</strong> shared with the host. Therefore, the host has to copy data to the device and copy s back when the kernel finishes.</li>

<li><p>Before doing this, the host must allocate global memory on the device and, afterwards, free it again, like <code>malloc</code> and <code>free</code>.</p>

<ul>
<li><p><strong>Note: the return of <code>cudaMalloc</code> is an error number</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>h_A <span style="color:#f92672">=</span> (<span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>)malloc(size);
<span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>h_B <span style="color:#f92672">=</span> (<span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>)malloc(size);
<span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>d_A <span style="color:#f92672">=</span> NULL;
err <span style="color:#f92672">=</span> cudaMalloc((<span style="color:#66d9ef">void</span> <span style="color:#f92672">**</span>)<span style="color:#f92672">&amp;</span>d_A, size);
<span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>d_C <span style="color:#f92672">=</span> NULL;
err <span style="color:#f92672">=</span> cudaMalloc((<span style="color:#66d9ef">void</span> <span style="color:#f92672">**</span>)<span style="color:#f92672">&amp;</span>d_C, size);
...
err <span style="color:#f92672">=</span> cudaMemcpy(d_A,h_A, size, cudaMemcpyHostToDevice);
... <span style="color:#75715e">// Invoke kernel
</span><span style="color:#75715e"></span>err <span style="color:#f92672">=</span> cudaMemcpy(h_c,d_C, size, cudaMemcpyDeviceToHost);
...
err <span style="color:#f92672">=</span> cudaFree(d_A);
err <span style="color:#f92672">=</span> cudaFree(d_C);</code></pre></div></li>
</ul></li>
</ul>

<p><strong>Note: this code is for a slightly different example, use it only as an example of <code>cuda&lt;method&gt;</code> methods</strong></p>

<h2 id="cuda-error-handling">CUDA Error Handling</h2>

<p>The only way to check hat things are working correctly on the GPU is the check the error return values. <strong>Check them every time</strong></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#66d9ef">if</span> (err <span style="color:#f92672">!=</span> cudaSuccess){
    <span style="color:#75715e">// handler
</span><span style="color:#75715e"></span>}</code></pre></div>
<ul>
<li>Kernel functions don&rsquo;t return error numbers. However, after it has finished you can call <code>err = cudaGetLastError();</code> to get the error number if one occurred.</li>
<li>Since kernel functions can run in parallel with host functions, if ou call <code>cudaGetLastError()</code> before the kernel function finishes, the rror may only occur after you requested the error.</li>
<li>If you really want to avoid this call <code>cudaDeviceSyncronize()</code>

<ul>
<li>But avoid this as it is very inefficient.</li>
</ul></li>
</ul>

<h2 id="timing-host-code-with-host-timers">TIming Host Code with Host Timers</h2>

<ul>
<li><p>Typically we want to time both the sequential code (on the host) and the parallel code (on the GPU).</p></li>

<li><p>The general approach to time the <strong>Host</strong> code is:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#75715e"># include &lt;cuda_runtime.h&gt;
</span><span style="color:#75715e"># include &lt;helper_cuda.h&gt;
</span><span style="color:#75715e"># include &lt;helper_functions.h&gt;
</span><span style="color:#75715e"></span>
StopWatchInterface <span style="color:#f92672">*</span>timer <span style="color:#f92672">=</span> NULL;
sdkCreateTimer(<span style="color:#f92672">&amp;</span>timer);
sdkStartTimer(<span style="color:#f92672">&amp;</span>timer);

<span style="color:#75715e">/* The Host code that is to be timed*/</span>

sdkStopTimer(<span style="color:#f92672">&amp;</span>timer);
<span style="color:#66d9ef">double</span> h_msecs <span style="color:#f92672">=</span> sdkGetTimerValue(<span style="color:#f92672">&amp;</span>timer);
sdkDeleteTimer(<span style="color:#f92672">&amp;</span>timer);</code></pre></div></li>

<li><p>In general do <strong>NOT</strong> use host timers to time GPU code. They are much less accurate.</p></li>

<li><p>However, if you <strong>have to</strong> time the <strong>Host and GPU</strong> code using host timers the pattern is:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c"><span style="color:#75715e"># include &lt;cuda_runtime.h&gt;
</span><span style="color:#75715e"># include &lt;helper_cuda.h&gt;
</span><span style="color:#75715e"># include &lt;helper_functions.h&gt;
</span><span style="color:#75715e"></span>
StopWatchInterface <span style="color:#f92672">*</span>timer <span style="color:#f92672">=</span> NULL;
sdkCreateTimer(<span style="color:#f92672">&amp;</span>timer);
sdkStartTimer(<span style="color:#f92672">&amp;</span>timer);

<span style="color:#75715e">/* Host + GPU code that is to be timed */</span>

cudaDeviceSyncronize();

sdkStopTimer(<span style="color:#f92672">&amp;</span>timer);
<span style="color:#66d9ef">double</span> h_msecs <span style="color:#f92672">=</span> sdkGetTimerValue(<span style="color:#f92672">&amp;</span>timer);
sdkDeleteTimter(<span style="color:#f92672">&amp;</span>timer);</code></pre></div></li>

<li><p>The best way to time GPU code is to insert an <strong>event</strong> into the GPU execution stream before and after the code to time and get the elapsed time from them.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-c" data-lang="c">cudaEvent_t start, stop;
<span style="color:#66d9ef">float</span> m_secs; 
cudaEventCreate(<span style="color:#f92672">&amp;</span>start);
cudaEventCreate(<span style="color:#f92672">&amp;</span>stop);

cudaEventRecord(start ,<span style="color:#ae81ff">0</span>);

<span style="color:#75715e">/* Call GPU kernel(s) */</span>

cudaEventRecord(stop, <span style="color:#ae81ff">0</span>);
cudaEventSyncronize(stop);

cudaEventElapsedTime(<span style="color:#f92672">&amp;</span>d_msecs, start, stop);
cudaEventDestroy(start);
cudaEventDestroy(stop);</code></pre></div></li>
</ul>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="/Notes/modules/neural-computation/">Neural Computation</a>
        
    
    
        
            &#183; 
            <a href="/Notes/modules/machine-learning/">Machine Learning</a>
        
            &#183; 
            <a href="/Notes/modules/parallel-distributed/">Distributed and Parallel Computing</a>
        
    
    &#183; 
    <a href="https://sam-barrett.codes/Notes/">
        main
    </a>

</p></div>
        

        <div class="section footer"></div>
    </div>
</body>

</html>