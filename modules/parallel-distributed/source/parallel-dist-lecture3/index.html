<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="My University Notes Site.">
<title>
 - Welcome to my Notes Repo!
</title>




<link rel="shortcut icon" href="/Notes/sam.ico">








<link rel="stylesheet" href="/Notes/css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sam-barrett.codes/Notes/tn.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Lecture 3 DRAM Access Patterns    Rule Ideal Values Description     Granularity 6 - 64 B Amount of data transferred in a single request     Reading smaller sizes is very inefficient   Locality 1- 4kB If you use an address now, you will likely use it again soon (fetch from cache, not memory)     If you use an address now, you will likely use one close to it soon (In same cache line, if realated objects are stored too far away from eachother, the cache line can be flushed causing a memory read (inefficient)   L1, L2 caching 64-256 kB Set of bytes read/written repeatedly is stored here until replaced, subsequent reads and writes hit the cache not memory, much faster."/>

<meta property="og:title" content="" />
<meta property="og:description" content="Lecture 3 DRAM Access Patterns    Rule Ideal Values Description     Granularity 6 - 64 B Amount of data transferred in a single request     Reading smaller sizes is very inefficient   Locality 1- 4kB If you use an address now, you will likely use it again soon (fetch from cache, not memory)     If you use an address now, you will likely use one close to it soon (In same cache line, if realated objects are stored too far away from eachother, the cache line can be flushed causing a memory read (inefficient)   L1, L2 caching 64-256 kB Set of bytes read/written repeatedly is stored here until replaced, subsequent reads and writes hit the cache not memory, much faster." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sam-barrett.codes/Notes/modules/parallel-distributed/source/parallel-dist-lecture3/" />

<meta property="og:image" content="https://sam-barrett.codes/Notes/tn.png" />
<meta property="og:site_name" content="Notes" />


    

    
    
    
    <title>
        
        
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title"></div>

        <div class="section" id="content">

<h1 id="lecture-3">Lecture 3</h1>

<h2 id="dram-access-patterns">DRAM Access Patterns</h2>

<table>
<thead>
<tr>
<th>Rule</th>
<th>Ideal Values</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>Granularity</td>
<td>6 - 64 B</td>
<td>Amount of data transferred in a single request</td>
</tr>

<tr>
<td></td>
<td></td>
<td>Reading smaller sizes is very inefficient</td>
</tr>

<tr>
<td>Locality</td>
<td>1- 4kB</td>
<td>If you use an address now, you will likely use it again soon (fetch from cache, not memory)</td>
</tr>

<tr>
<td></td>
<td></td>
<td>If you use an address now, you will likely use one close to it soon (In same cache line, if realated objects are stored too far away from eachother, the cache line can be flushed causing a memory read (inefficient)</td>
</tr>

<tr>
<td>L1, L2 caching</td>
<td>64-256 kB</td>
<td>Set of bytes read/written repeatedly is stored here until replaced, subsequent reads and writes hit the cache not memory, much faster. Shared between 2 threads</td>
</tr>

<tr>
<td>L3 Caching</td>
<td>8-20MB</td>
<td>&rdquo;&rdquo;__&ldquo;&rdquo;. Shared between all cores</td>
</tr>
</tbody>
</table>

<h3 id="hit-ratio">Hit Ratio</h3>

<ul>
<li>The Hit Ratio is the proportion of request that hit in the cache</li>
<li>It is a measure of effectiveness of the caching mechanism</li>
<li>Depends on locality of application</li>
</ul>

<h3 id="cache-flushing-re-filling">Cache Flushing/ Re-filling</h3>

<ul>
<li>When a cache becomes full and needs to make room for a new entry it must first evict an old one</li>
<li>Needs a replacement policy

<ul>
<li>Random Works well in practice</li>
</ul></li>
</ul>

<h3 id="cache-topologies">Cache Topologies</h3>

<h4 id="fully-associative-caches">Fully Associative Caches</h4>

<ul>
<li>Any line can be anywhere in the cache

<ul>
<li>Advantage: Can replace any line</li>
<li>Disadvantage: Hard to find specific lines</li>
</ul></li>
</ul>

<h4 id="direct-mapped-cache">Direct Mapped Cache</h4>

<ul>
<li>Every address has exactly one slot

<ul>
<li>Advantage: easy to find a line</li>
<li>Disadvantage: must replace an entire fixed line</li>
</ul></li>
</ul>

<h4 id="k-way-set-associative-cache">K-Way Set Associative Cache</h4>

<ul>
<li>Each slot holds <em>k</em> lines

<ul>
<li>Advantage : relatively easy to find a line</li>
<li>Disadvantage: Some choice needed in replacing a line</li>
</ul></li>
</ul>

<h4 id="cache-coherence">Cache Coherence</h4>

<ul>
<li>Given 2 cores/ threads <strong>A</strong> and <strong>B</strong></li>
<li>If <strong>A</strong> and <strong>B</strong> both cache address $x$</li>
<li><strong>A</strong> writes to $x$

<ul>
<li>updates cache</li>
</ul></li>
<li>How does <strong>B</strong> find out?</li>
<li>There are many different protocols</li>
</ul>

<h5 id="mesi">MESI</h5>

<ul>
<li>Modified

<ul>
<li>Any modified cache data must be written back to memory</li>
</ul></li>
<li>Exclusive

<ul>
<li>Not modified, <em>I</em> have the only copy</li>
</ul></li>
<li>Shared

<ul>
<li>Not modified, may be cached elsewhere</li>
</ul></li>
<li>Invalid

<ul>
<li>Cache contents is not meaningful</li>
</ul></li>
<li><strong>See slide deck for diagrammatic explanation</strong></li>
</ul>

<h5 id="write-through-caches">Write-through Caches</h5>

<ul>
<li>Immediately broadcast changes

<ul>
<li>Advantages:</li>
<li>Memory and caches always agree</li>
<li>May lead to more read hits</li>
<li>Disadvantages:</li>
<li>Bus traffic on all writes</li>
<li>Most writes to unshared data

<ul>
<li>e.g. loop indexes</li>
</ul></li>
</ul></li>
</ul>

<h5 id="write-back-caches">Write-back Caches</h5>

<ul>
<li>Accumulate changes in cache</li>
<li>Write back when line evicted

<ul>
<li>Occurs when either:</li>
<li>Cache space is needed for something else</li>
<li>Another processor wants the data</li>
</ul></li>
</ul>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="/Notes/modules/neural-computation/">Neural Computation</a>
        
    
    
        
            &#183; 
            <a href="/Notes/modules/machine-learning/">Machine Learning</a>
        
            &#183; 
            <a href="/Notes/modules/parallel-distributed/">Distributed and Parallel Computing</a>
        
    
    &#183; 
    <a href="https://sam-barrett.codes/Notes/">
        main
    </a>

</p></div>
        

        <div class="section footer"></div>
    </div>
</body>

</html>