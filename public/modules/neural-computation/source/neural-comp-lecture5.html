<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="My University Notes Site.">
<title>
 - Welcome to my Notes Repo!
</title>




<link rel="shortcut icon" href="../../../sam.ico">








<link rel="stylesheet" href="../../../css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sam-barrett.codes/Notes/tn.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Lecture 5 Computation Graphs We will describe ML models using computation graphs where
 Nodes represent variables (values, vectors, matrices) Edges represent functional dependencies  i.e. an edge from $x$ to $y$ indicates that $y$ is a function of $x$   Example The linear regression model, $z = \sum_{i=1}^m a_iw_i&#43;b$ could be represented as:
Feed-Forward Neural Network Stepping it up, here is a simple Feed-forward Neural Network:
Where:"/>

<meta property="og:title" content="" />
<meta property="og:description" content="Lecture 5 Computation Graphs We will describe ML models using computation graphs where
 Nodes represent variables (values, vectors, matrices) Edges represent functional dependencies  i.e. an edge from $x$ to $y$ indicates that $y$ is a function of $x$   Example The linear regression model, $z = \sum_{i=1}^m a_iw_i&#43;b$ could be represented as:
Feed-Forward Neural Network Stepping it up, here is a simple Feed-forward Neural Network:
Where:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sam-barrett.codes/Notes/modules/neural-computation/source/neural-comp-lecture5.html" />

<meta property="og:image" content="https://sam-barrett.codes/Notes/tn.png" />
<meta property="og:site_name" content="Notes" />


    

    
    
    
    <title>
        
        
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title"></div>

        <div class="section" id="content">

<h1 id="lecture-5">Lecture 5</h1>

<h2 id="computation-graphs">Computation Graphs</h2>

<p>We will describe ML models using <strong>computation graphs</strong> where</p>

<ul>
<li><em>Nodes</em> represent variables (values, vectors, matrices)</li>
<li><em>Edges</em> represent functional dependencies

<ul>
<li>i.e. an edge from $x$ to $y$ indicates that $y$ is a function of $x$</li>
</ul></li>
</ul>

<h3 id="example">Example</h3>

<p>The linear regression model, $z = \sum_{i=1}^m a_iw_i+b$ could be represented as:</p>

<p><img src="../resources/BasnCompGraph.jpg" alt="Basic Computation Graph" /></p>

<h2 id="feed-forward-neural-network">Feed-Forward Neural Network</h2>

<p>Stepping it up, here is a <em>simple</em> Feed-forward Neural Network:</p>

<p><img src="../resources/FFNeuralNet.jpg" alt="Feed-Forward Neural Network" /></p>

<p>Where:</p>

<ul>
<li>$L \leftarrow$ number of layers in the network, where

<ul>
<li>layer 1 is the <em>&ldquo;input layer&rdquo;</em></li>
<li>layer $L$ is the <em>&ldquo;output layer&rdquo;</em></li>
</ul></li>
<li>$m\leftarrow$ is the <em>&ldquo;width&rdquo;</em> of the network, which can change for each layer</li>
<li>$w_{jk}^l\leftarrow$ is the <em>&ldquo;weight&rdquo;</em> of the connection between the $k^{th}$ unit in layer $l-1$, to the $j^{th}$ unit in layer $l$</li>
<li>$b_j^l\leftarrow$ is the <em>&ldquo;bias&rdquo;</em> of the $j^{th}$ unit in layer $l$

<ul>
<li>*<strong>Note:</strong> These appear for every $z$ node, for illustrative purposes I have only shown them on the first bank.*</li>
</ul></li>
<li>$z_j^l = \sum<em>k w</em>{jk}^l a_k^{l-1} + b_j^l \leftarrow$ is the weighted input unit $j$ in layer $l$</li>
<li>$a_j^l = \phi(z_j^l)\leftarrow$ is the <em>&ldquo;activation&rdquo;</em> of unit $j$ in layer $l$, where $\phi$ is an <em>&ldquo;activation function&rdquo;</em>.
<br /></li>
</ul>

<h3 id="training-of-feed-forward-neural-networks">Training of Feed-Forward Neural Networks</h3>

<p>The parameters of the network are:</p>

<ul>
<li>The weights $w_{jk}^l$ in each layer</li>
<li>The Biases $b_j^l$</li>
</ul>

<p><img src="../resources/TrainNeuralNet.jpg" alt="Training FF-Neural Nets" /></p>

<h4 id="general-idea">General Idea</h4>

<p>To apply gradient descent to optimise a weight $w$ (or bias $b$) in a network,we apply the chain rule</p>

<h6 id="1">(1)</h6>

<p>$$\frac{\delta C}{\delta w} = \frac{\delta C}{\delta v}\cdot\frac{\delta v}{\delta w}$$</p>

<p><img src="../resources/ldeatrainNN.jpg" alt="Idea Application" /></p>

<h6 id="2">(2)</h6>

<p>$$z<em>j^l = \sum</em>{k=1}^m w_{jk}^l a_k^{l-1}+ b_j^l$$</p>

<h6 id="3">(3)</h6>

<p>$$a_j^l = \phi(z_j^l)$$</p>

<h6 id="4">(4)</h6>

<p>$$\frac{\delta C}{\delta w_{jk}^l} = \frac{\delta C}{\delta z_j^l}\cdot\frac{\delta z<em>j^l}{\delta w</em>{jk}^l} = \frac{\delta C}{\delta z_j^l}\cdot a_k^{l-1}$$</p>

<p>Where $a_k^{l-1}$ is the <em>&ldquo;activation&rdquo;</em> of unit $k$ in layer $l-1$</p>

<h6 id="5">(5)</h6>

<p>$$\frac{\delta C}{\delta b_j^l} = \frac{\delta C}{\delta z_j^l}\cdot \frac{\delta z_j^l}{\delta b_j^l} = \frac{\delta C}{\delta z_j^l}\cdot 1$$</p>

<p>Hence, we can compute $\frac{\delta C}{\delta w_{kj}}$ and $\frac{\delta C}{\delta b_j^l}$ if we know:</p>

<h6 id="6">(6)</h6>

<p>$$\delta_j^l := \frac{\delta C}{\delta z_j^l}$$</p>

<p>The vector $\delta^l$ is called the <strong>local gradient</strong> for layer $l$</p>

<h5 id="local-gradient-for-output-layers">Local Gradient for output layers</h5>

<p><img src="../resources/GradOutput.jpg" alt="Local Gradient for L=L" /></p>

<h6 id="7">(7)</h6>

<p>$$a_j^L = \phi(z_j^L)$$</p>

<p>The local gradient for the output layer is:</p>

<h6 id="8">(8)</h6>

<p>$$\delta_j^L = \frac{\delta C}{\delta z_j^L}$$
<a href="#6">By definition</a></p>

<p>$$= \frac{\delta C}{\delta a_j^L}\cdot\frac{\delta a_j^L}{\delta z_j^L}$$
By the chain rule</p>

<p>$$= \frac{\delta C }{\delta a_j^L} \cdot \phi&rsquo;(z_j^L)$$
Because $a_j^L = \phi(z_j^L)$</p>

<p>The partial derivative $\frac{\delta C}{\delta a_j^L}$ depends on the cost function.
For example, for a regression problem in $m$ dimensions, one could define :</p>

<h6 id="9">(9)</h6>

<p>$$C(a_1^L,\cdots,a<em>m^L) := \frac{1}{2}\sum</em>{k=1}^m (y_k - a_k^L)^2$$
Where:</p>

<ul>
<li>$y_k$ is the desired output in the $k^{th}$ dimension</li>

<li><p>$a<em>k^L$ is the predicted output in the $k</em>{th}$ dimension</p></li>

<li><p><strong>i.e. this is essentially mean squared error</strong></p></li>
</ul>

<p>in which case,</p>

<h6 id="10">(10)</h6>

<p>$$\frac{\delta C}{\delta a_j^L}= a_j^L - y_j$$</p>

<h5 id="local-gradient-for-hidden-layers">Local Gradient for Hidden Layers</h5>

<p><img src="../resources/GradHiddenLayer.jpg" alt="Hidden Layer Local Gradient" /></p>

<h6 id="10-1">(10)</h6>

<p>$$z_k^{l+1} = \sum<em>r w</em>{kr}^{l+1} a_r^l$$</p>

<p>$$a_j^l = \phi(z_j^l)$$</p>

<h6 id="11">(11)</h6>

<p>$$\delta_j^l = \frac{\delta C}{\delta z_j^l}$$
By <a href="#6">definition of local gradient $\delta_j^l$</a>
$$= \frac{\delta C}{\delta a_j^l}\cdot \frac{\delta a_j^l}{\delta z_j^l}$$
By chain rule
$$= \left(\sum_k \frac{\delta C}{\delta z_k^{l+1}} \cdot \frac{\delta z_k^{l+1}}{\delta a_j^l}\right)\cdot \phi&rsquo;(z_j^l)$$
By chain rule with respect to $\frac{\delta C}{\delta a_j^l}$</p>

<p>$$= \phi&rsquo;(z_j^l)\sum_k \delta<em>k^{l+1}\cdot w</em>{kj}^{l+1}$$
By [definition of local gradient $\delta_k^{l+1}$]()</p>

<h5 id="summary">Summary</h5>

<p>$\forall$ weights, $w$ and biases $b$</p>

<h6 id="12">(12)</h6>

<p>$$\frac{\delta C}{\delta w_{jk}^l} = \delta_j^l\cdot a_k^{l-1}$$</p>

<h6 id="13">(13)</h6>

<p>$$\frac{\delta C}{\delta b_j^l} = \delta_j^l$$</p>

<p>Where the local gradient $\delta_j^l$ is:</p>

<h6 id="14">(14)</h6>

<p>$$\delta_j^l =
\begin{cases}
\phi&rsquo;(z_j^L)\cdot \frac{\delta C}{\delta a_j^L} &amp;\text{if $l=L$ (output layer)} \<br />
\phi&rsquo;(z_j^l)\cdot\sum_k \delta<em>k^{l+1}\cdot w</em>{kj}^{l+1} &amp;\text{otherwise (hidden layer)}
\end{cases}$$</p>

<h3 id="matrix-description">Matrix Description</h3>

<p>The back-propagation algorithm can exploit efficient implementations of matrix operations</p>

<p><em>Note: recall that for a matrix $\textbf{A} \in \R^m\times\R^n$, $A_ij$ denotes the element in the $i^{th}$ row and $j^{th}$ column</em></p>

<p>For two matrices $\textbf{A} \in \R^m\times\R^n$ and $\textbf{B} \in \R^n\times\R^l$
| Operation                                                    | Name                  |
| &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash; | &mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash; |
| $(\textbf{A}^T)<em>{ij} = A</em>{ji}$                               | Matrix Transpose      |
| $(\textbf{AB})_{ij} = \sum<em>k \textbf{A}</em>{ik}\textbf{B}_{kj}$ | Matrix Multiplication |</p>

<p>For two vectors $\vec{u},\vec{v} \in \R^m$</p>

<table>
<thead>
<tr>
<th>Operation</th>
<th>Name</th>
</tr>
</thead>

<tbody>
<tr>
<td>$\vec{u}+\vec{v} = \langle u_1+v_1,\cdots,u_m+v_m\rangle$</td>
<td>Vector addition</td>
</tr>

<tr>
<td>$\vec{u}\cdot\vec{v} = \sum_{i=1}^m u_iv_i $</td>
<td>Dot Product</td>
</tr>

<tr>
<td>$\vec{u}\odot\vec{u} = \langle u_1v_1,\cdots,u_mv_m \rangle$</td>
<td>Hadamard Product</td>
</tr>

<tr>
<td>$(\vec{u}\vec{v}^T)_{ij} = u_iv_j$</td>
<td>Outer Product</td>
</tr>
</tbody>
</table>

<h4 id="weighted-inputs-and-activations">Weighted Inputs and Activations</h4>

<h6 id="15">(15)</h6>

<p>$$z^l = (z_1^l, \cdots, z_m^l) $$</p>

<p>$$= \left( \sum<em>{k=1}^m w</em>{1k}^la_k^{l-1} + b<em>1^l, \cdots, \sum</em>{k=1}^m w_{mk}^la_k^{l-1}+ b_m^l \right)$$</p>

<p>$$= w^la^l-1 + b $$</p>

<h6 id="16">(16)</h6>

<p>$$a^l = (a_1^l,\cdots, a_m^l)$$</p>

<p>$$= (\phi(z_1^l), \cdots, \phi(z_m^l))$$</p>

<p>$$=\phi(z^l)$$</p>

<h4 id="local-gradients">Local Gradients</h4>

<h5 id="output-layer">Output Layer</h5>

<h6 id="17">(17)</h6>

<p>$$\delta^L = (\delta_1^L,\cdots, \delta_m^L)$$</p>

<p>$$= \left( \frac{\delta C}{\delta a_1^L}\cdot \phi&rsquo;(z_1^L), \cdots, \frac{\delta C}{\delta a_m^L}\cdot\phi&rsquo;(z_m^L) \right)$$</p>

<p>$$= \nabla_{a^L}C \odot \phi&rsquo;(z^L)$$</p>

<h5 id="hidden-layer">Hidden Layer</h5>

<h6 id="18">(18)</h6>

<p>$$\delta^l = (\delta_1^l, \cdots, \delta_m^l)$$</p>

<p>$$= \left( \phi&rsquo;(z_1^l)\cdot\sum_k \delta<em>k^{l+1}\cdot w</em>{k1}^{l+1} , \cdots, \phi&rsquo;(z_m^l)\cdot \sum_k \delta<em>k^{l+1}\cdot w</em>{km}^{l+1}\right)$$</p>

<p>$$= \phi&rsquo;(z^l) \odot \left( \sum<em>k (w^{l+1})</em>{1k}^T\delta_k^{l+1},\cdots, \sum<em>k(w^{l+1})</em>{mk}^T\delta_k^{l+1}\right)$$</p>

<p>$$= \phi&rsquo;(z^l)\odot (w^{l+1})^T\delta^{l+1}$$</p>

<h3 id="backpropagation-algorithm">Backpropagation Algorithm</h3>

<p><u>Input</u> : A training example, $(x,y)\in \R^m\times\R^{m&rsquo;}$</p>

<ol>
<li>Set the activation in the input layer
$a^1 = x$</li>
<li>for each $l=(2\cdots L)$, feed forward
$z^l = w^la^{l-1}+b^l$
$a^l = \phi(z^l)$</li>
<li>Compute local gradient for output layer
$\delta^L := \nabla_{a^L}C \odot \phi&rsquo;(z^L)$</li>
<li>Backpropagate local gradients for hidden layers, i.e
For each $l=(L-1 \cdots 2)$
$\hspace{10px}\delta^l := \left( (w^{l+1})^T\delta^{l+1}\right)\odot \phi&rsquo;(z^l)$</li>
<li>return the partial derivatioes
$\frac{\delta C}{\delta w_{jk}^l} = a_k^{l-1}\delta_j^l$
$\frac{\delta C}{\delta b_j^l} = \delta_j^l$</li>
</ol>

<h3 id="training-feed-forward-neural-networks">Training Feed-Forward Neural Networks</h3>

<p>Assume $n$ training samples</p>

<p>$$(x_1,y_1),\cdots,(x_n,y_n)$$</p>

<p>and a cost function:</p>

<p>$$C = \frac{1}{m}\sum_{i=1}^n C_i$$</p>

<p>Where $C_i$ is the cost on the $i^{th}$ example.</p>

<p>For example, with Mean Squared error, we can define it as:</p>

<p>$$C_i = \frac{1}{2}(y_i-a^L)$$</p>

<p>Where $a^L$ is the output of the network when $a^1 = x_i$</p>

<p>Backpropagation gives us the gradient of the overall cost function as follows:</p>

<h6 id="19">(19)</h6>

<p>$$\frac{\delta C}{\delta w^l} = \frac{1}{m}\sum_{i=1}^n \frac{\delta C_i}{\delta w^l}$$</p>

<p>$$\frac{\delta C}{\delta b^l} = \frac{1}{m}\sum_{i=1}^n \frac{\delta C_i}{\delta B^l}$$</p>

<p><strong>Note: these provide the <em>average</em> gradient per training example</strong></p>

<p>We can now use gradient descent to optimise the weights, $w$ and biases, $b$.</p>

<h4 id="mini-batch-gradient-descent">Mini-Batch Gradient Descent</h4>

<p>Computing the gradients is expensive when the number of training examples, $n$ is large</p>

<p>We can approximate the gradients:</p>

<h6 id="20">(20)</h6>

<p>$$\frac{\delta C}{\delta w^l} \approx \frac{1}{b}\sum_{i=1}^b \frac{\delta C_i}{\delta w^l}$$</p>

<p>$$\frac{\delta C}{\delta b^l} \approx \frac{1}{b} \sum_{i=1}^b \frac{\delta C_i}{\delta b^l}$$</p>

<p>using a random <em>&ldquo;mini-batch&rdquo;</em> of $b\leq n$ training examples</p>

<table>
<thead>
<tr>
<th>Size</th>
<th>Name</th>
</tr>
</thead>

<tbody>
<tr>
<td>$1&lt;b&lt;n$</td>
<td>Mini-batch Gradient Descent</td>
</tr>

<tr>
<td>$b=1$</td>
<td>Stochastic Gradient Descent</td>
</tr>

<tr>
<td>$b=n$</td>
<td>Batch Gradient Descent</td>
</tr>
</tbody>
</table>

<p>It is common to use mini-batch size of $b \in (20,100)$</p>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="../../../modules/neural-computation/">Neural Computation</a>
        
    
    
        
            &#183; 
            <a href="../../../modules/machine-learning/">Machine Learning</a>
        
            &#183; 
            <a href="../../../modules/parallel-distributed/">Distributed and Parallel Computing</a>
        
    
    &#183; 
    <a href="https://sam-barrett.codes/Notes/">
        main
    </a>

</p></div>
        

        <div class="section footer"></div>
    </div>
</body>

</html>