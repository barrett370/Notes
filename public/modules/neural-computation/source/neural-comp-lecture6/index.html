<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="My University Notes Site.">
<title>
 - Welcome to my Notes Repo!
</title>




<link rel="shortcut icon" href="/Notes/sam.ico">








<link rel="stylesheet" href="/Notes/css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sam-barrett.codes/Notes/tn.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Lecture 6 Softmax In Lecture 5 we derived the per-example cost function:
$$Ci = \sum{j=1}^m\frac{1}{2}(y_j^{(i)} -a_j^L)^2 \tag{1}$$
where $a_j^L$ is the output of the model
Using the maximum likelihood method under the assumption that the predicted output $a_j^L$ has a Gaussian (normal) distribution. This assumption is acceptable for regression problems
However, when we look at classification problems with $m$ discrete class labels ${1,\cdots m}$ it makes more sense to have one output unit $P_j$ per class, $j$ where $P_j$ is the probability of the example falling into class $j$"/>

<meta property="og:title" content="" />
<meta property="og:description" content="Lecture 6 Softmax In Lecture 5 we derived the per-example cost function:
$$Ci = \sum{j=1}^m\frac{1}{2}(y_j^{(i)} -a_j^L)^2 \tag{1}$$
where $a_j^L$ is the output of the model
Using the maximum likelihood method under the assumption that the predicted output $a_j^L$ has a Gaussian (normal) distribution. This assumption is acceptable for regression problems
However, when we look at classification problems with $m$ discrete class labels ${1,\cdots m}$ it makes more sense to have one output unit $P_j$ per class, $j$ where $P_j$ is the probability of the example falling into class $j$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sam-barrett.codes/Notes/modules/neural-computation/source/neural-comp-lecture6/" />

<meta property="og:image" content="https://sam-barrett.codes/Notes/tn.png" />
<meta property="og:site_name" content="Notes" />


    

    
    
    
    <title>
        
        
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title"></div>

        <div class="section" id="content">

<h1 id="lecture-6">Lecture 6</h1>

<h1 id="softmax">Softmax</h1>

<p>In <a href="../out/Neural-Comp-Lecture5.html">Lecture 5</a> we derived the per-example cost function:</p>

<p>$$C<em>i = \sum</em>{j=1}^m\frac{1}{2}(y_j^{(i)} -a_j^L)^2 \tag{1}$$</p>

<p>where $a_j^L$ is the output of the model</p>

<p>Using the maximum likelihood method under the assumption that the predicted output $a_j^L$ has a Gaussian (normal) distribution. This assumption is acceptable for <strong><em>regression problems</em></strong></p>

<p>However, when we look at <strong><em>classification problems</em></strong> with $m$ discrete class labels ${1,\cdots m}$ it makes more sense to have one output unit $P_j$ per class, $j$ where $P_j$ is the probability of the example falling into class $j$</p>

<p>These units therefore have to satisfy the following conditions:</p>

<ul>
<li>$1 \geq P_j \geq 0 \forall j \in {1\cdots,m}$</li>
<li>$\sum_{j=1}^mP_j = 1$</li>
</ul>

<p>Diagrammatically, we replace the last layer of our network with a <em>&ldquo;softmax&rdquo;</em> layer</p>

<p><img src="../resources/SoftMax.jpg" alt="Softmax Diagram" /></p>

<p>$$P_j = \frac{\mathcal{e}^{z_j^L}}{Q} \tag{2}$$</p>

<p>where:</p>

<p>$$Q =\sum_{j=1}^m e^{z_j^L}\tag{3}$$</p>

<p>Here we use the exponential function to make certain that our probability stays positive and that the total probability doesn&rsquo;t exceed 1, a quick proof of this is as follows:</p>

<p>$$\sum_{j=1}^m P<em>j = \frac{1}{Q}\sum</em>{j=1}^m e^{z_i^L} = \frac{Q}{Q} = 1 \tag{4}$$</p>

<p>Then, as we have bounded our probabilities, we can say:</p>

<p>$$P<em>y = P</em>{wb}(y|x) \tag{5}$$</p>

<p>i.e the probability of class, $y\in{1,\cdots,m}$ given input $x$</p>

<h2 id="defining-our-loss-function-mathcal-l">Defining our loss function $\mathcal{L}$</h2>

<p>Given $n$ independent observations $(x_1,y_1),\cdots, (x_n,y_n)$ where $y_i \in {1,\cdots,n}$ is the class corresponding to the input $x_i$, the likelihood of weigth and bias parameters $w,b$ is:</p>

<p>$$\mathcal{L}(w,b | (x_1,y_1),\cdots, (x_n,y<em>n)) = \prod</em>{i=1}^n P_{wb}(y_i | x<em>i) \ = \prod</em>{i=1}^n P_{y_i} \tag{6}$$
&lt;!&ndash;
When we have a continuous pdf we describe the probabilities with the product of the densities</p>

<p>When we have a discrete pdf we use the product of the probabilities &ndash;&gt;</p>

<p>We can define a cost function using the maximum likelihood principal.</p>

<p>$$C = -\log \mathcal{L}(w,b | (x_1,y_1),\cdots,(x_n,y<em>n)) \
= \frac{1}{n}\sum</em>{i=1}^n C_i \tag{7}$$</p>

<p>where:</p>

<p>$$C<em>i = -\log P</em>{wb}(y_i | x<em>i) \ = -\log P</em>{y<em>i} \ = \log Q - z</em>{y_i}^L \tag{8}$$</p>

<p>To apply gradient descent to minimise the cost function, we need to compute the gradients.</p>

<p>$$\frac{\delta C}{\delta w<em>{jk}^l} = \sum</em>{i=1}^n \frac{\delta C<em>i}{\delta w</em>{jk}^l} \tag{9}$$</p>

<p>$$\frac{\delta C}{\delta b<em>{j}^l} = \sum</em>{i=1}^n \frac{\delta C<em>i}{\delta b</em>{j}^l} \tag{10}$$</p>

<p>To compute these using backpropogation, we need to compute the local gradient for the softmax layer:</p>

<p>$$\delta_j^L := \frac{\delta C_i}{\delta z_j} \tag{11}$$</p>

<h2 id="theorem">Theorem</h2>

<p>$$\delta_j^L = \frac{\delta C_i}{\delta z_j^L} = P<em>j - \delta</em>{y_ij}$$</p>

<p>where
$$\delta_{ab} =
\begin{cases}
1 &amp; \text{if $a=b$} <br />
0 &amp; \text{otherwise}
\end{cases}$$</p>

<ul>
<li>This is also known as the <em>Kronecker-Delta function</em></li>
</ul>

<h2 id="proof">Proof</h2>

<p>$$\frac{\delta C_i}{\delta z_j^L} = -\frac{\delta\log P(y_i | x_i)}{\delta z<em>j} \
= - \frac{\delta \log P</em>{y_i}}{\delta z_j} \
= - \frac{\delta}{\delta z<em>j}\left( z</em>{y<em>i} - \log Q\right) \
= - \left( \delta</em>{y_ij} - \frac{\delta \log Q}{\delta z<em>j}\right) <br />
= - \left( \delta</em>{y_ij} - \frac{1}{Q}e^{z_j}\right) \
= P<em>j - \delta</em>{y_ij} <br />
\square \tag{12}$$</p>

<h2 id="numerical-issues-with-softmax">Numerical Issues with Softmax</h2>

<p>Neural networks are usually implemented with fixed length representations of real numbers.
Obviously, these representations are finite representations of a uncountably infinite set $\R$</p>

<p>For instance, the maximal value of the <code>float64</code> data type in NumPy can represent is $\approx 1.8\times10^{308}$
When computing the numerator in $\frac{e^{z_j^L}}{Q}$ we can easily exceed this limit</p>

<h3 id="note">Note:</h3>

<p>For any constant,$r$:</p>

<p>$$P_j = \frac{e^{z_j^L}}{\sum_k e^{z_k^L}} = \frac{e^r\cdot e^{z_j^L}}{e^r\cdot \sum_k e^{z_k^L}} = \frac{e^{z_j^L + r}}{\sum_k e^{z_k^L + r}} \tag{13}$$</p>

<p>To avoid too large exponents, it is common to implement the softmax function as the rightmost expansion above with the constant:</p>

<p>$$r := -\max_k z_k^L \tag{14}$$</p>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="/Notes/modules/neural-computation/">Neural Computation</a>
        
    
    
        
            &#183; 
            <a href="/Notes/modules/machine-learning/">Machine Learning</a>
        
            &#183; 
            <a href="/Notes/modules/parallel-distributed/">Distributed and Parallel Computing</a>
        
    
    &#183; 
    <a href="https://sam-barrett.codes/Notes/">
        main
    </a>

</p></div>
        

        <div class="section footer"></div>
    </div>
</body>

</html>