<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="My University Notes Site.">
<title>
 - Welcome to my Notes Repo!
</title>




<link rel="shortcut icon" href="../../../sam.ico">








<link rel="stylesheet" href="../../../css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sam-barrett.codes/Notes/tn.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Lecture 8 Repetition: Gradient Descent In gradient descent we have an input of a cost function, $\mathcal{J}$ and a learning rate, $\varepsilon$
$$ \mathcal{J} : \R^m \rightarrow \R $$
The pseudo-code for conventional gradient decent is as follows:
foo
$$ x\leftarrow \text{some initial point in } \R^m \text{while termination condition not met} \ { x \leftarrow x - \varepsilon \cdot \nabla \mathcal{J}(x) \ } $$
Impact of learning rate $\varepsilon$ on Stochastic Gradient Descent  Learning rate in SGD can have a marked impact on the optimisation time of a network Often necessary to vary/ adjust the learning rate according to the specific model you are training."/>

<meta property="og:title" content="" />
<meta property="og:description" content="Lecture 8 Repetition: Gradient Descent In gradient descent we have an input of a cost function, $\mathcal{J}$ and a learning rate, $\varepsilon$
$$ \mathcal{J} : \R^m \rightarrow \R $$
The pseudo-code for conventional gradient decent is as follows:
foo
$$ x\leftarrow \text{some initial point in } \R^m \text{while termination condition not met} \ { x \leftarrow x - \varepsilon \cdot \nabla \mathcal{J}(x) \ } $$
Impact of learning rate $\varepsilon$ on Stochastic Gradient Descent  Learning rate in SGD can have a marked impact on the optimisation time of a network Often necessary to vary/ adjust the learning rate according to the specific model you are training." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sam-barrett.codes/Notes/modules/neural-computation/source/neural-comp-lecture8.html" />

<meta property="og:image" content="https://sam-barrett.codes/Notes/tn.png" />
<meta property="og:site_name" content="Notes" />


    

    
    
    
    <title>
        
        
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title"></div>

        <div class="section" id="content">

<h1 id="lecture-8">Lecture 8</h1>

<h2 id="repetition-gradient-descent">Repetition: Gradient Descent</h2>

<p>In gradient descent we have an input of a cost function, $\mathcal{J}$ and a learning rate, $\varepsilon$</p>

<p>$$
\mathcal{J} : \R^m \rightarrow \R
$$</p>

<p>The pseudo-code for conventional gradient decent is as follows:</p>

<p>foo</p>

<p>$$
x\leftarrow \text{some initial point in } \R^m <br />
\text{while termination condition not met} \ { <br />
    x \leftarrow x - \varepsilon \cdot \nabla \mathcal{J}(x) \
}
$$</p>

<h3 id="impact-of-learning-rate-varepsilon-on-stochastic-gradient-descent">Impact of learning rate $\varepsilon$ on Stochastic Gradient Descent</h3>

<ul>
<li>Learning rate in SGD can have a marked impact on the optimisation time of a network</li>

<li><p>Often necessary to vary/ adjust the learning rate according to the specific model you are training.</p></li>

<li><p>If a learning rate that is too high is used then the model parameters vary wildly causing a large variance in the value of the loss function, $\mathcal{J}$</p></li>
</ul>

<p><img src="../resources/thlr.png" alt="Case 1: Too high learning rate" /></p>

<ul>
<li>If too low a learning rate is used then the model stagnates far from the optimum conditions, it may converge given enough epochs but this would take exponentially long relative to the size of the learning rate</li>
</ul>

<p><img src="../resources/tllr.png" alt="Case 2: Too low learning rate" /></p>

<h2 id="gradient-descent-with-momentum">Gradient Descent with Momentum</h2>

<p>$$
\text{Choose initial parameter, } \Theta <br />
v = 0  <br />
\text{while termination condition is not met} \ { <br />
v = \alpha v - \mathcal{E}\nabla_{\Theta}C(\Theta) <br />
\Theta = \Theta + v \ }
$$</p>

<p>Conceptually, we can think of momentum in this context the same as in physics:</p>

<blockquote>
<p>A Ball with position $\Theta$ and a velocity $v$ influenced by two forces, one which pushes the ball in the opposite direction of the current gradient, and a     viscous drag determined by a parameter $\alpha &lt; 1 $</p>
</blockquote>

<p>The momentum <em>smooths out</em> the update steps. The size of the updates is proportional to how similar the previous gradients are.</p>

<h3 id="two-hyper-parameters">Two Hyper-parameters</h3>

<ul>
<li>$\varepsilon \leftarrow$ learning rate</li>
<li>$\alpha \leftarrow$ Factor which determines the influence of the past gradients on the current update of the parameter, typical values include 0.5, 0.9 or 0.99</li>
</ul>

<h2 id="nesterov-momentum">Nesterov Momentum</h2>

<p>$$
\text{Choose an initial parameter, } \Theta <br />
v = 0 <br />
\text{While termination condition is not met}\ { <br />
v = \alpha v - \varepsilon\nabla C(\Theta+ \alpha v ) <br />
\Theta = \Theta + v <br />
}
$$</p>

<p>Nesterov Momentum is a variant of standard momentum where the gradient is computed after the velocity is applied</p>

<p><img src="../resources/nesterov.png" alt="Nesterov momentum on a hyper-plane" /></p>

<h2 id="adagrad">Adagrad</h2>

<blockquote>
<p>Duchi et al. 2011</p>
</blockquote>

<p>$$
\text{choose an initial parameter, } \Theta <br />
r = 0 <br />
\text{while termination condition is not met} { <br />
g = \nabla_{\Theta}C(\Theta) <br />
r = r + g \odot g \text{$\leftarrow$ <em>Squared gradient</em> } <br />
v = \frac{\varepsilon}{\delta + \sqrt{r}} \odot g <br />
\Theta = \Theta + v
$$</p>

<p>Where $\delta$ is a hyperparameter, typically around $10^{-6}$</p>

<p>Adagrad adapts a (possibly differing) <em>learning rate</em> $\frac{\varepsilon}{\delta + \sqrt{r}}$ for each dimension according to accumulated square gradient, $r$</p>

<ul>
<li>Large $r \implies$ small $\frac{\varepsilon}{\delta + \sqrt{r}}$</li>
<li>Small $r \implies$ large $\frac{\varepsilon}{\delta + \sqrt{r}}$</li>
</ul>

<p><strong>AdaGrad works well for problems with sparse gradients</strong></p>

<p>All gradients are weighted equally by $r$</p>

<h2 id="adam">Adam</h2>

<blockquote>
<p>Kingma and Ba, 2014</p>
</blockquote>

<p>$$
\text{Choose an initial parameter, } \Theta <br />
r = 0, s = 0, t = 0 <br />
\text{while termination condition not met} {  <br />
t += 1  <br />
g = \nabla_{\Theta} C(\Theta) <br />
s = \rho_1s + (1-\rho_1)g <br />
r = \rho_2r + (1-\rho_2)g\odot g <br />
\overset{n}{s} = \frac{s}{1-\rho_1^t} , \overset{n}{r} = \frac{r}{1-\rho_2^t} <br />
v = -\varepsilon \cdot \frac{\overset{n}{s}}{\sqrt{\overset{n}{r}}+ \delta} <br />
\Theta = \Theta + v
$$</p>

<p>Here $v$ is defined as the average gradient divided by the average squared gradient</p>

<p>Typical hyperparameter values are as follows:</p>

<ul>
<li>$\varepsilon = 0.001$</li>
<li>$\rho_1 = 0.9$</li>
<li>$\rho_2 = 0.999$</li>
<li>$\delta = 10^{-8}$</li>
</ul>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="../../../modules/neural-computation/">Neural Computation</a>
        
    
    
        
            &#183; 
            <a href="../../../modules/machine-learning/">Machine Learning</a>
        
            &#183; 
            <a href="../../../modules/parallel-distributed/">Distributed and Parallel Computing</a>
        
    
    &#183; 
    <a href="https://sam-barrett.codes/Notes/">
        main
    </a>

</p></div>
        

        <div class="section footer"></div>
    </div>
</body>

</html>