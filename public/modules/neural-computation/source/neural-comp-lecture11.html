<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="My University Notes Site.">
<title>
 - Welcome to my Notes Repo!
</title>




<link rel="shortcut icon" href="../../../sam.ico">








<link rel="stylesheet" href="../../../css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sam-barrett.codes/Notes/tn.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Lecture 11: Deep Learning Linear Regression - Revisit In linear regression problems we have the following:
 Our data, $(x_1,y_1),\ldots, (x_n,y_n)$ Our model, $y = \textbf{w}x &#43; \textbf{b}$ A loss function, often the mean squared error:  $$ L(x,\theta) = \sum_{i=1}^{n}|(\textbf{w}x_i &#43; \bf{b}) - y_i|^2 $$
Where, $\theta = { \textbf{w},\textbf{b}}$
 An optimiser with underlying optimisation function, often gradient descent:  $$ \boldsymbol{\theta}^{j&#43;1} = \boldsymbol{\theta}^j - \nabla_\theta L(x,\boldsymbol{\theta}^j) $$"/>

<meta property="og:title" content="" />
<meta property="og:description" content="Lecture 11: Deep Learning Linear Regression - Revisit In linear regression problems we have the following:
 Our data, $(x_1,y_1),\ldots, (x_n,y_n)$ Our model, $y = \textbf{w}x &#43; \textbf{b}$ A loss function, often the mean squared error:  $$ L(x,\theta) = \sum_{i=1}^{n}|(\textbf{w}x_i &#43; \bf{b}) - y_i|^2 $$
Where, $\theta = { \textbf{w},\textbf{b}}$
 An optimiser with underlying optimisation function, often gradient descent:  $$ \boldsymbol{\theta}^{j&#43;1} = \boldsymbol{\theta}^j - \nabla_\theta L(x,\boldsymbol{\theta}^j) $$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sam-barrett.codes/Notes/modules/neural-computation/source/neural-comp-lecture11.html" />

<meta property="og:image" content="https://sam-barrett.codes/Notes/tn.png" />
<meta property="og:site_name" content="Notes" />


    

    
    
    
    <title>
        
        
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title"></div>

        <div class="section" id="content">

<h1 id="lecture-11-deep-learning">Lecture 11: Deep Learning</h1>

<h2 id="linear-regression-revisit">Linear Regression - Revisit</h2>

<p>In linear regression problems we have the following:</p>

<ul>
<li>Our data, $(x_1,y_1),\ldots, (x_n,y_n)$</li>
<li>Our model, $y = \textbf{w}x + \textbf{b}$</li>
<li>A loss function, often the mean squared error:</li>
</ul>

<p>$$
L(x,\theta) = \sum_{i=1}^{n}|(\textbf{w}x_i + \bf{b}) - y_i|^2
$$</p>

<p>Where, $\theta = { \textbf{w},\textbf{b}}$</p>

<ul>
<li>An optimiser with underlying optimisation function, often gradient descent:</li>
</ul>

<p>$$
\boldsymbol{\theta}^{j+1} = \boldsymbol{\theta}^j - \nabla_\theta L(x,\boldsymbol{\theta}^j)
$$</p>

<ul>
<li>Our final step is <em>inference</em>, once we have the optimal parameters, we can simply plug in these values to allow us to accurately infer the correct result of unseen values.</li>
</ul>

<h2 id="mulilayer-perceptrons">Mulilayer Perceptrons</h2>

<p>Networks are made up of a series of banks of artificial neurons. Each bank feeds into the following layer until the final <em>output</em> layer.
In our neurons we need to define a function which determines how and when the neuron <em>fires</em>. These are known as activation functions.</p>

<h3 id="activation-functions">Activation Functions</h3>

<h4 id="sigmoid">Sigmoid</h4>

<p>$$
f(x) = \frac{1}{(1+e^{-x})}
$$</p>

<!-- > True shape -->

<iframe src="https://www.desmos.com/calculator/mspmujvsgi?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>

<!-- > Illustration
@import "../resources/sigmoidfunc.png" -->

<ul>
<li>The sigmoid function normalises numbers to the range [0,1]</li>
<li>As x varies from 0 the gradient of the function tends towards 0, this is a major drawback as it makes it much more difficult/ slower for your network to <em>learn</em></li>
<li>Another downside of the sigmoid function is due to its nature as an <em>exponential</em> function, it is quite computationally expensive.</li>
</ul>

<h4 id="tanh">$\tanh$</h4>

<p>$$
f(x) = \tanh(x)
$$
<!-- > True shape -->
<iframe src="https://www.desmos.com/calculator/srw2rd3opp?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe></p>

<!-- > Illustration
@import "../resources/tanhfunc.png" -->

<ul>
<li>This function normalises values to [-1,1]</li>
<li>This suffers from the same issue as the sigmoid due to the fact that its gradient also tends to 0 meaning in some cases learning can come to a halt.</li>
</ul>

<h4 id="relu-rectified-linear-unit">ReLU - Rectified Linear Unit</h4>

<p>$$
f(x) = \max(0,x)
$$</p>

<!-- >True shape -->

<iframe src="https://www.desmos.com/calculator/ickbylfkrq?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>

<ul>
<li>Solves the issue of saturation leading to slowed or halted learning, but <strong>only</strong> in the positive region.

<ul>
<li>You must guarantee that your result is positive for this to work</li>
<li>If you do not then you get a <em>dead ReLU</em> which has no gradient so therefore, no learning</li>
</ul></li>
<li>Converges much faster than sigmoid and tanh ~ 6x</li>
</ul>

<h4 id="leaky-relu">Leaky ReLU</h4>

<p>$$
f(x) = \max(0.01x,x)
$$
<!-- > true shape -->
<iframe src="https://www.desmos.com/calculator/i1sao0xykc?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe></p>

<!-- > Illustration
@import "../resources/leakyrelufunc.png" -->

<ul>
<li>Avoids saturation in both positive and negative regions, i.e. will not produce a 0 gradient in any scenario</li>
<li>Similar speed to that of standard ReLU</li>
</ul>

<h3 id="tips-in-practice">Tips in Practice</h3>

<ul>
<li>ReLU is most common</li>
<li>Try Leaky ReLU in certain circumstances</li>
<li>Only try tanh or sigmoid as last layer</li>
</ul>

<h3 id="design">Design</h3>

<p>A multilayer perceptron is comprised of layers or banks of neurons that feed into each other and eventually into an output layer of 1 neuron or a softmax function for probabilistic networks.</p>

<p>The first layer can the thought of as the input data, the following $n-2$ layers are the <em>hidden layers</em> which learn the optimal parameters. The final layer is the output layer producing the probability or label for a given sample</p>

<h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2>

<h3 id="dot-product-and-convolution">Dot Product and Convolution</h3>

<p>The dot product of $\vec{a} = \lang a_1,\ldots,a_n \rang$ and $\vec{b} = \lang b_1,\ldots, b_n\rang$ is defined as:</p>

<p>$$
\vec{a}\cdot\vec{b} = \sum_{i=1}^{n}a_ib_i = a_1b_1 + a_2b_2 + \cdots + a_nb_n
$$</p>

<p>The convolution between an image $x$ and a kernel $\textbf{w}$ is given as:</p>

<p>$$
G = \textbf{w} * x\;\;\;\;\;\;\;  G[i,j] = \sum<em>{u=-k}^{k}\sum</em>{v=-k}^{k}\textbf{w}[u,v]x[i-u,j-v]
$$</p>

<blockquote>
<p>Convolution is also sometimes denoted as $\circledast$</p>
</blockquote>

<p>Where $u$ and $v$ are indices in the kernel grid and $i$ and $j$ are indices in the image grid. $k$ denotes the radius of the kernel</p>

<p>Convolutional Neural Networks employ this convolution function to extract and learn features from input image data.</p>

<p>Kernels exist for edge detection, sharpening and blur. Networks often learn kernels that are a combination of many of these in order to extract complex features from the images.</p>

<h2 id="data-pre-processing">Data Pre-processing</h2>

<p>The idea behind pre-possessing our input data is to make our loss function and resulting loss less sensitive to changes in the model parameters as this makes it hard to optimise our model.</p>

<p>We will look at 2 different normalisation methods:</p>

<h3 id="min-max">Min-Max</h3>

<p>$$
\text{Min-Max} = \frac{\text{value}-\text{min}}{\text{max}-\text{min}}
$$</p>

<p>@import &ldquo;../resources/min-max-norm.png&rdquo;</p>

<p>This method of normalisation guarantees all features will have the exact same scale but does not handle outliers well.</p>

<h3 id="z-score">Z-Score</h3>

<p>$$
\text{Z-Score} = \frac{\text{value}-\text{mean}}{\text{std}}
$$</p>

<p>$$
\text{Z-Score} = \frac{x-\bar{x}}{\sigma}
$$</p>

<p>@import &ldquo;../resources/z-score-norm.png&rdquo;</p>

<p>This method handles outliers well burt does not produce normalised data with the exact same scale.</p>

<h3 id="pca-principal-component-analysis">PCA - Principal Component Analysis</h3>

<ol>
<li>By performing principal component analysis on the original data we centre the data at 0 and rotate into the eigenbasis of the data&rsquo;s covariance matrix. In so doing, we de-correlate the data</li>
<li>Each dimension is additionally scaled by eigenvalues, transforming the data&rsquo;s covariance matrix into the identity matrix. Geometrically, this essentially corresponds to stretching and squeezing the data into an isotropic gaussian <em>blob</em></li>
</ol>

<p>@import &ldquo;../resources/pca.png&rdquo;</p>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="../../../modules/neural-computation/">Neural Computation</a>
        
    
    
        
            &#183; 
            <a href="../../../modules/machine-learning/">Machine Learning</a>
        
            &#183; 
            <a href="../../../modules/parallel-distributed/">Distributed and Parallel Computing</a>
        
    
    &#183; 
    <a href="https://sam-barrett.codes/Notes/">
        main
    </a>

</p></div>
        

        <div class="section footer"></div>
    </div>
</body>

</html>