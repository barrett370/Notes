<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="My University Notes Site.">
<title>
 - Welcome to my Notes Repo!
</title>




<link rel="shortcut icon" href="../../../sam.ico">








<link rel="stylesheet" href="../../../css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sam-barrett.codes/Notes/tn.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Lecture 3: Maximum Likelihood  So far we have considered a deterministic model, $f(x) = wx$    However, we can see that there is variation in the data for each value of $x$ A probabilistic model can account for this variance  e.g. $F(x) = wx &#43; N$ where: $N \sim \mathcal{N}(0,\sigma^2)$ is a noise term $F(X)$ is a random variable which can be described by a conditional density $P(y | x, w)$    An aside into basic probability Probability Density Functions  A random variable takes a value that depends on a random phenomenon The density function ofa continuous random variable $X$ is a function $p : \R \rightarrow \R$ s."/>

<meta property="og:title" content="" />
<meta property="og:description" content="Lecture 3: Maximum Likelihood  So far we have considered a deterministic model, $f(x) = wx$    However, we can see that there is variation in the data for each value of $x$ A probabilistic model can account for this variance  e.g. $F(x) = wx &#43; N$ where: $N \sim \mathcal{N}(0,\sigma^2)$ is a noise term $F(X)$ is a random variable which can be described by a conditional density $P(y | x, w)$    An aside into basic probability Probability Density Functions  A random variable takes a value that depends on a random phenomenon The density function ofa continuous random variable $X$ is a function $p : \R \rightarrow \R$ s." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sam-barrett.codes/Notes/modules/neural-computation/source/neural-comp-lecture3.html" />

<meta property="og:image" content="https://sam-barrett.codes/Notes/tn.png" />
<meta property="og:site_name" content="Notes" />


    

    
    
    
    <title>
        
        
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title"></div>

        <div class="section" id="content">

<h1 id="lecture-3-maximum-likelihood">Lecture 3: Maximum Likelihood</h1>

<ul>
<li>So far we have considered a <strong>deterministic model</strong>, $f(x) = wx$</li>
</ul>

<!-- ![Cat Heart dataset](../resources/cat-heart-0.png) -->

<div style="text-align:center"><img src="../resources/cat-heart-0.png" /></div>

<ul>
<li>However, we can see that there is <strong>variation</strong> in the data for each value of $x$</li>
<li>A <strong>probabilistic model</strong> can account for this variance

<ul>
<li>e.g. $F(x) = wx + N$</li>
<li>where:</li>
<li>$N \sim \mathcal{N}(0,\sigma^2)$</li>
<li>is a <em>noise term</em></li>
<li>$F(X)$ is a <em>random variable</em> which can be described by a <em>conditional density</em> $P(y | x, w)$
<br /></li>
</ul></li>
</ul>

<hr />

<h2 id="an-aside-into-basic-probability">An aside into basic probability</h2>

<h3 id="probability-density-functions">Probability Density Functions</h3>

<ul>
<li>A <strong>random variable</strong> takes a value that depends on a random phenomenon</li>
<li>The <strong>density function</strong> ofa continuous random variable $X$ is a function $p : \R \rightarrow \R$ s.t.

<ul>
<li>$\int_a^b p(x) \delta x = Pr(a \leq x \leq b) $</li>
</ul></li>
</ul>

<p><img src="../resources/Norm-Dist.jpg" alt="Normal Distribution Curve" /></p>

<ul>
<li>The normal distribution has the probability density function</li>
</ul>

<h6 id="1">(1)</h6>

<p>$$f(x ; \mu , \sigma^2) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp(-\frac{(x - \mu)^2}{2\sigma^2})$$</p>

<ul>
<li>Where $\mu$ and $\sigma^2$ are the parameters of the distribution.</li>
</ul>

<h3 id="expectation">Expectation</h3>

<ul>
<li>The <strong>expected value</strong> of $f(x)$ when $x$ is a random variable with <em>p.d.f</em> $P$ is</li>
</ul>

<h6 id="2">(2)</h6>

<p>$$\mathbb{E}<em>{x\sim P}[f(x)] = \int</em>{-\infin}^{\infin} P(x)f(x) \delta x$$</p>

<h3 id="joint-distributions-and-independence">Joint Distributions and Independence</h3>

<ul>
<li>The <strong>joint density function</strong> of $n$ random variables $x_1, \cdots, x_n$ is a function $P : \R^n \rightarrow \R$ s.t.</li>
</ul>

<h6 id="3">(3)</h6>

<p>$$\int_D P(x_1,\cdots, x_n)\delta x_1 \cdots \delta x_n = Pr((x_1 ,\cdots, x_n)\in D)$$</p>

<ul>
<li><p>for any $n$-dimensional domain $D \subseteq \R^n $</p></li>

<li><p>if $x_1,\cdots, x_n$ are $n$ <strong>independent</strong> random variables with density functions $P_1, \cdots, P_n$ and joint density $P$ then</p></li>
</ul>

<h6 id="4">(4)</h6>

<p>$$P_{\theta}(x_1,\cdots, x<em>n) = \prod</em>{i=1}^n P_{\theta}(x_i)$$</p>

<h3 id="empirical-distribution">Empirical Distribution</h3>

<ul>
<li>Given $n$ independent samples $X_i, \cdots, X_n$ from an unknown distribution, $\mathcal{D}$, we can construct an <em>approximation</em> of $\mathcal{D}$ by uniformly sampling from the set ${X_1, \cdots, X_n}$</li>
</ul>

<p><img src="../resources/Emp-Dist.jpg" alt="Empirical Distribution Sampling" /></p>

<ul>
<li>Given $X_1, \cdots , X_n$ initial samples from $\mathcal{D}$, the <strong>empirical distribution</strong> of $\mathcal{D}$ has the density function:</li>
</ul>

<h6 id="5">(5)</h6>

<p>$${Pr}^n(x) := \frac{1}{n}\sum_{i=1}^{n} \delta(X_i - x)$$</p>

<ul>
<li>Where $\delta$ is the <em>Divac delta</em> i.e. $\delta(x) = 0$ for $x\neq 0$ and $\int_{-\infin}^\infin \delta(x) \delta x = 1$</li>
</ul>

<p><strong>Note: $\mathbb{E}<em>{X\sim{Pr}^n}[f(X)] = \frac{1}{n}\sum</em>{i=1}^n f(X_i)$</strong></p>

<h3 id="the-learning-task-t">The Learning Task, $T$</h3>

<ul>
<li>Instead of deterministically predicting an output $y$ for a given input $x$ we will now train a probabilistic model represented by a conditional density function</li>
</ul>

<h6 id="6">(6)</h6>

<p>$$P_{model}(y | x ; \theta)$$</p>

<ul>
<li><p>Where:</p>

<ul>
<li>$y\leftarrow$  density function of output</li>
<li>$x\leftarrow$  input</li>
<li>$\theta \leftarrow$ parameter of model</li>
</ul></li>

<li><p>Given training data and a <em>family</em> of probability models we need to choose the parameter(s) $\theta$ which are most appropriate for the data. We call this the <strong>Maximum likelihood estimate</strong></p></li>
</ul>

<h3 id="likelihood-function">Likelihood function</h3>

<ul>
<li>Given independent training data $(x_1,y_1), \cdots, (x_n,y<em>n)$ and a probabilistic model $P</em>{model}$ with parameter $\theta$, the <strong>likelihood function</strong> is defined as:</li>
</ul>

<h6 id="7">(7)</h6>

<p>$$\mathcal{L}(\theta; (x_1,y_1), \cdots , (x_n,y<em>n)) := \prod</em>{i=1}^n P_{model}(y_i | x_i ; \theta)$$</p>

<ul>
<li>$\mathcal{L}(\theta; &hellip;)$ is the <em>likelihood</em> that the observed data came from the model with parameter $\theta$</li>
</ul>

<h3 id="maximum-likelihood-estimate-mle">Maximum Likelihood Estimate (MLE)</h3>

<ul>
<li>Given training data and a family of models indexed by the parameter $\theta$, <em>which of the models are most likely to have produce the data?</em></li>
</ul>

<h6 id="8">(8)</h6>

<p>$$\Theta<em>{MLE} := \argmax</em>\theta \mathcal{L}(\theta; (x_1,y_1),\cdots, (x_n,y<em>n)) = \argmax</em>\theta \prod<em>{i=1}^n P</em>{model}(y_i | x_i; \theta)$$</p>

<h4 id="log-likelihood">Log-Likelihood</h4>

<ul>
<li>For numerical and analytical reason, a convenient reformation is:</li>
</ul>

<h6 id="9">(9)</h6>

<p>$$\Theta<em>{MLE} = \argmax</em>\theta \mathcal{L}(\theta) <br />
= \argmax<em>\theta \log \mathcal{L(\theta)} \
= \argmax</em>\theta \log \Pi<em>{i=1}^n P</em>{model}(y_i | x<em>i ; \theta) <br />
= \argmax</em>\theta \sum<em>{i=1}^n \log P</em>{model} (y_i | x<em>i ; \theta) \
= \argmin</em>\theta \frac{1}{n}\sum<em>{i=1}^n -\log P</em>{model} (y_i | x<em>i ; \theta) \
= \argmin</em>\theta \mathbb{E}<em>{(\mathcal{X},\mathcal{Y})\sim \mathcal{D}^n} - \log P</em>{model} (\mathcal{Y} | \mathcal{X} ; \theta) $$</p>

<h5 id="learning-via-log-likelihood">Learning via Log-Likelihood</h5>

<ul>
<li>Neural network models are often trained by minimising the negative log-likelihood of the model given the training data, i.e. by minimising:</li>
</ul>

<h6 id="10">(10)</h6>

<p>$$J(\theta) = \mathbb{E}<em>{\mathcal{X},\mathcal{Y}\sim \mathcal{D}^n}- \log P</em>{model}(\mathcal{Y | X} ;\theta)$$</p>

<ul>
<li>Where:

<ul>
<li>$J(\theta)\leftarrow$ Cost function</li>
<li>$\theta \leftarrow$ model parameter(s)</li>
<li>$\mathcal{D}^n \leftarrow$ empirical distribution of data</li>
</ul></li>
</ul>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="../../../modules/neural-computation/">Neural Computation</a>
        
    
    
        
            &#183; 
            <a href="../../../modules/machine-learning/">Machine Learning</a>
        
            &#183; 
            <a href="../../../modules/parallel-distributed/">Distributed and Parallel Computing</a>
        
    
    &#183; 
    <a href="https://sam-barrett.codes/Notes/">
        main
    </a>

</p></div>
        

        <div class="section footer"></div>
    </div>
</body>

</html>