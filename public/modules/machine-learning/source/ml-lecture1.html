<!DOCTYPE html>
<html>

<head>
    
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="referrer" content="no-referrer">
<meta name="description" content="My University Notes Site.">
<title>
 - Welcome to my Notes Repo!
</title>




<link rel="shortcut icon" href="../../../sam.ico">








<link rel="stylesheet" href="../../../css/main.min.81bbafc4df93b11c1c3e2449464373c384aa4903731b4fc7a77dfcdd979e184f.css" integrity="sha256-gbuvxN&#43;TsRwcPiRJRkNzw4SqSQNzG0/Hp3383ZeeGE8=" crossorigin="anonymous" media="screen">



 

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Didact+Gothic">

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://sam-barrett.codes/Notes/tn.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Lecture 1 Regression  The task of finding the relationship (mathematical function) between one or more numerical inputs (independent variables) and one or more numerical outputs (dependent variables)
 Curve fitting
 Given a set of points, try ot learn a function to describe them Given a value $x$, we can predict the corresponding value $y$ Not just for straight line fitting   Simple example Let us consider a simple linear example with 1 independent variable, $x$ &amp; 1 dependent variable, $y$"/>

<meta property="og:title" content="" />
<meta property="og:description" content="Lecture 1 Regression  The task of finding the relationship (mathematical function) between one or more numerical inputs (independent variables) and one or more numerical outputs (dependent variables)
 Curve fitting
 Given a set of points, try ot learn a function to describe them Given a value $x$, we can predict the corresponding value $y$ Not just for straight line fitting   Simple example Let us consider a simple linear example with 1 independent variable, $x$ &amp; 1 dependent variable, $y$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sam-barrett.codes/Notes/modules/machine-learning/source/ml-lecture1.html" />

<meta property="og:image" content="https://sam-barrett.codes/Notes/tn.png" />
<meta property="og:site_name" content="Notes" />


    

    
    
    
    <title>
        
        
        
    </title>
</head>

<body>
    <div class="wrap">
        <div class="section" id="title"></div>

        <div class="section" id="content">

<h1 id="lecture-1">Lecture 1</h1>

<h2 id="regression">Regression</h2>

<ul>
<li><p>The task of finding the relationship <em>(mathematical function)</em> between one or more numerical inputs <em>(independent variables)</em> and one or more numerical outputs <em>(dependent variables)</em></p></li>

<li><p>Curve fitting</p>

<ul>
<li>Given a set of points, try ot learn a function to describe them</li>
<li>Given a value $x$, we can predict the corresponding value $y$</li>
<li>Not just for straight line fitting</li>
</ul></li>
</ul>

<h3 id="simple-example">Simple example</h3>

<p>Let us consider a simple <em>linear</em> example with 1 independent variable, $x$ &amp; 1 dependent variable, $y$</p>

<h6 id="1">(1)</h6>

<p>$$\mathcal{D} = {(x_1,y_1),&hellip;,(x_n,y_n)} = {(x_i,y<em>i)}</em>{i=1}^N$$</p>

<p>Model the relationship between $x$ and $y$ with the function $f(\textbf{w},x)$, s.t $y_i \approx f(\textbf{w},x_i)$
Measurements of $y$, subject ot noise are defined by,</p>

<h6 id="2">(2)</h6>

<p>$$y_i = f(\textbf{w},x_i) + \epsilon$$</p>

<p>Where $\epsilon$ is a random number drawn from some continuous probability density function.
Goal is to find some $\textbf{w}$ that solves, or provides the best approximation to the above equation.</p>

<p>First, let us approach this as a optimisation problem in which the objective is to find the value of <strong>w</strong> (denoted <strong>w</strong>*) that minimises some <em>loss</em> or objective function $L(\textbf{w})$</p>

<h6 id="3">(3)</h6>

<p>$$\textbf{w}* = \argmin_w L(\textbf{w}) $$</p>

<p>Intuitively, $L(\textbf{w})$ should be designed to capture the difference between the data and the predictions of the model, and seek to minimise this.
One common choice for $L(\textbf{w})$ is <em>least-squares error</em>. Given our dataset $D$ and modelling function $f(\textbf{w},x)$, we construct $\forall d \in \mathcal{D}$ a residual error defined as:</p>

<h6 id="4">(4)</h6>

<p>$$ r_i(\textbf{w}) = y_i - f(\textbf{w}, x_i)$$</p>

<!-- ![Graph showing residuals](../resources/residuals.png) -->

<div style="text-align:center"><img src="../resources/residuals.png" /></div>

<p>The least squares error <em>(LSE)</em> loss function is defined in terms of residuals as:</p>

<h6 id="5">(5)</h6>

<p>$$ L<em>{LSE}(\textbf{w}) = \sum</em>{i=1}^N r_i^2 = \textbf{r}^T\textbf{r} $$</p>

<p>It is important to note that the above definition implicitly has no upper bound but is restricted to being strictly posive, thus, allowing us to find a minimum value.</p>

<h6 id="6">(6)</h6>

<p>$$\textbf{w}^* = \argmin<em>\textbf{w} L</em>{LSE}(\textbf{w})$$</p>

<p>Optimisation is a very difficult problem to solve and so, for now, we resitct ourselves to a specifically <em>linear</em> case.</p>

<p><strong>N.B. when we say <em>linear</em> we do not reference the problem being of the form &ldquo;$y = mx + c$&rdquo; but instead that our problem is linear in it&rsquo;s *unknown parameters</strong>*</p>

<p>Linear models take the form:</p>

<h6 id="7">(7)</h6>

<p>$$f(\textbf{w},x) = w_0\phi<em>0(x) + \cdots + w</em>{M-1}\phi<em>{M-1}(x) = \sum</em>{i=0}^{M-1} w_i\phi_i(x)$$</p>

<p>Our function is a <em>linear combination</em> of a set of <em>basis functions</em>, ${\phi<em>i(x) }</em>{i=0}^{M-1}$ weighted by our free parameters ${w<em>i}</em>{i=0}^{M-1}$.</p>

<p>A common choice of basis function is the polynomials ${x^i}_{i=0}^{M-1}$</p>

<p>For a finite set of data points $\mathcal{D}$ we can re-write the equation for $f(\textbf{w},x)$ in matrix form by defining a matrix <strong>$\Phi$</strong> with components $\Phi_{ij} = \phi_j(x_i)$ which yields the equation:</p>

<h6 id="8">(8)</h6>

<p><strong>$$f(w) = \Phi w$$</strong></p>

<p>Where the variable $x$ is swallowed by the construction of <strong>$\Phi$</strong> in <strong>$f$</strong></p>

<h4 id="example-phi-construction">Example $\Phi$ Construction</h4>

<p>For a simple quadratic model $f(\textbf{w},x) = w_0 + w_1x+ w_2x^2$ with basis functions ${x^0,x^1,x^2} = {1,x,x^2}$, we construct:</p>

<h6 id="9">(9)</h6>

<p>$$\boldsymbol{\Phi} =
\begin{pmatrix}
1 &amp; x_1 &amp; x_1^2  <br />
1 &amp; x_2 &amp; x_2^2  <br />
\vdots &amp; \vdots &amp; \vdots <br />
1 &amp; x_N &amp; x_N^2
\end{pmatrix}$$</p>

<p>We can now begin to solve our <a href="#3">equation here</a>. The residuals <a href="#4">defined here</a> can be written as:</p>

<h6 id="10">(10)</h6>

<p><strong>$$r = y - \Phi w$$</strong></p>

<p>And our <a href="#6">loss function</a>, becomes:</p>

<h6 id="11">(11)</h6>

<p>$$L_{LSE}(\textbf{w}) = (\textbf{y}-\boldsymbol{\Phi}\textbf{w})^T(\textbf{y}-\boldsymbol{\Phi}\textbf{w})$$</p>

<p>As we have observed, $L<em>{LSE}$ has no upper bound but does have a lower bound. To minimise $L</em>{LSE}$ we find the point at which $L<em>{LSE}&rsquo; = 0$ where $L</em>{LSE}&lsquo;$ is it&rsquo;s first derivative with respect to it&rsquo;s free parameters.</p>

<p>If we differentiate $L_{LSE}$ with respect to $\textbf{w}$ we get:</p>

<h6 id="12">(12)</h6>

<p>$$\frac{\delta L_{LSE}(\textbf{w})}{\delta \textbf{w}} = -2 \boldsymbol{\Phi}^T(\textbf{y} - \boldsymbol{\Phi}\textbf{w})$$</p>

<p><strong>For a proof of this, see Iain Styles&rsquo; Notes</strong></p>

<p>Setting this result to 0 we obtain:</p>

<h6 id="13">(13)</h6>

<p>$$\boldsymbol{\Phi}^T\textbf{y} - \boldsymbol{\Phi}^T\boldsymbol{\Phi}\textbf{w}^*=0$$</p>

<p>This is known as the <strong>normal equations</strong> and are a set of simultaneous linear equations which we can solve for $\textbf{w}^*$</p>

<p><sub>*<em>These notes were heavily influenced by those of Dr. Iain Styles, University of Birmingham, School of Computer Science</em></sub></p>
</div>

        
        <div class="section bottom-menu">
<hr />
<p>


    

    
        
            <a href="../../../modules/neural-computation/">Neural Computation</a>
        
    
    
        
            &#183; 
            <a href="../../../modules/machine-learning/">Machine Learning</a>
        
            &#183; 
            <a href="../../../modules/parallel-distributed/">Distributed and Parallel Computing</a>
        
    
    &#183; 
    <a href="https://sam-barrett.codes/Notes/">
        main
    </a>

</p></div>
        

        <div class="section footer"></div>
    </div>
</body>

</html>